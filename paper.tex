
\documentclass[12pt, a4paper]{article}
\usepackage[onehalfspacing]{setspace}
\usepackage{amsmath}
\usepackage{mathrsfs}  
\usepackage{float}
\usepackage{varioref}
\usepackage[margin=1.1in]{geometry}
\usepackage[round]{natbib}


\usepackage[flushleft]{threeparttable}
\title{Predictive Regressions with Persistent Regressors:\\ Monte Carlo evidence for the Bonferroni Q-test}
\author{Jan Philipp Wöltjen  \\
	Seminar in Statistics, CAU, Kiel, Germany  \\
	}

\date{\today}
\begin{document}
\maketitle


\begin{abstract}
\citet{campbell2006efficient} propose a Bonferroni-type procedure for valid inference of predictive regressions with almost nonstationary regressors. In this paper, the rationale behind their method is explained. Empirical findings are replicated and extended with out-of-sample data that emerged since the method's publication. Furthermore, the method's behavior is investigate via Monte Carlo simulation, highlighting practically relevant scenarios where it over-rejects the null hypothesis of no predictability. 
\end{abstract}

\section{Introduction}
The question whether asset returns can be predicted with the help of observable variables is of great interest to financial institutions and the public as a whole. One of the most economically intuitive relationships exists between the price of a company's stock and the discounted earnings it will generate in the future. Since future earnings are not observable, current earnings or dividends or moving averages of them are often used as a proxy. To statistically test whether these variables are indeed predictive, one may test the significance of coefficients in a regression of the supposedly predictive variable onto equity market returns. The most straightforward way of doing this is via the t-test. \citet{elliott1994inference} find that this test has a nonstandard distribution under the null if the largest autoregressive root of the regressor is close to unity and the innovations of regressor and regressand are correlated. Given the presence of these nuisances, the t-test will lead to invalid inference. In this context, \citet{campbell2006efficient} develop a test that exploits information about the autoregressive root of the regressor and thus tries to overcome this issue. Section $\ref{regression}$ formalizes the regression setup and outlines the problem. Section $\ref{Derivation of the tests}$ explains the derivation of Campbell and Yogo's (2006) test. This is done by first considering optimal tests, albeit under unrealistic assumptions, in Section $\ref{Infeasible tests}$. This Section also establishes the conditions for a nonstandard distribution of the t-statistic. A pretest based on these conditions is described in Section $\ref{pretest}$. Section $\ref{Making the Q-test feasible}$ details the use of Bonferroni's inequality in making the test feasible.  The test is further generalized to more realistic assumptions in Section $\ref{implementation}$ and its mathematical implementation is described. Section $\ref{power}$ summarizes the power advantages of the test found by \citet{campbell2006efficient}. Section $\ref{Monte Carlo simulation}$ proceeds with evaluating finite-sample rejection rates of the feasible Q-test under various innovation distributions and violations of assumptions made in Section $\ref{implementation}$. First, Monte Carlo evidence of \citet{campbell2006efficient} is scrutinized and replicated in Section $\ref{Simulation with Gaussian innovations}$. Section $\ref{Simulation with weakly persistent regressors and Gaussian innovations}$ demonstrates over-rejection in finite-samples when the largest autoregressive root of the regressor is nonlocal-to-unity. An attempt of modeling innovations by a multivariate GARCH process is then made in Section $\ref{Simulation with CCC-GARCH innovations}$. This is further refined in Section $\ref{Simulation with common GARCH innovations}$ where it is hypothesized that innovation variances of both the regressor and the regressand share a common stochastic trend that is modeled by a GARCH process. Section $\ref{Simulation with common GJR-GARCH innovations}$ concludes the Monte Carlo evidence by simulation of the common stochastic trend by GJR-GARCH with conditional leptokurtosis. Empirical findings are replicated and evaluated on out-of-sample data in Section $\ref{Empirical Results}$. Finally, Section $\ref{conclusion}$ concludes.

\section{The regression setup and its problem}
\label{regression}
With log-returns $r_{t}$ observable at time $t$ and lagged supposedly predictive variable $x_{t-1}$ observable at $t-1$,  \citet{campbell2006efficient} consider the regression system
\begin{align} 
\label{eqn:1}
r_{t} = \alpha + \beta x_{t-1} + u_{t} \\ 
\label{eqn:2}
x_{t} = \gamma + \rho  x_{t-1} + e_{t}.
\end{align}
They further assume normality, i.e., 
\begin{equation}
w_{t}=\left(u_{t}, e_{t}\right)^{\prime} \stackrel{i i d}{\sim} \mathrm{N}(\boldsymbol{0}, \boldsymbol{\Sigma}),
\end{equation} where  $\boldsymbol{\Sigma}=\left[\begin{array}{cc}{\sigma_{u}^{2}} & {\sigma_{u e}} \\ {\sigma_{u e}} & {\sigma_{e}^{2}}\end{array}\right]$ is known.


Whether $x_{t-1}$ is indeed predictive may be ascertained by evaluating the statistically significance of the $\beta$ coefficient obtained from an ordinary least squares [OLS] regression, i.e., testing the null hypothesis $H_0: \beta = 0$. A naïve approach is to perform a t-test of $\beta$. \citet{elliott1994inference}, however, show that the t-test leads to invalid inference if $x_{t}$ is persistent, i.e., if $\rho$ is close to unity, and the noise terms $u_{t}$ and $e_{t}$ are highly correlated. Since the regressor is a valuation ratio, which is a function of the stock price, and the regressand is the stock's return, which is also a function of the current price, there is strong reason to believe that $u_{t}$ and $e_{t}$ will indeed be highly (negatively) correlated. Sample correlations, $\hat{\delta}$, in Table~\vref{tab:cy_sp} will confirm this belief. It will also be seen that the regressor is highly persistent, manifested by the 90\% confidence interval for the largest autoregressive root containing unity. Hence, worry about the validity of the t-test is justified. A pretest, testing the null of the actual size of the t-test being unacceptably larger than nominal size $\alpha$ based on the joint presence of both aforementioned conditions, is explained in more detail in Section $\ref{pretest}$.

\section{Derivation of the tests}
\label{Derivation of the tests}
\subsection{Infeasible tests}
\label{Infeasible tests}
In developing their test, \citet{campbell2006efficient} start by considering the t-statistic 
\begin{equation}
t\left(\beta_{0}\right)=\frac{\widehat{\beta}-\beta_{0}}{\sigma_{u}\left(\sum_{t=1}^{T} x_{t-1}^{\mu 2}\right)^{-1 / 2}},
\end{equation}
where $x_{t-1}^{\mu}=x_{t-1}-T^{-1} \sum_{t=1}^{T} x_{t-1}$ denotes the demeaned regressor and $\widehat{\beta}$ is the OLS estimate of $\beta$. They argue that this test ignores information contained in the system ($\ref{eqn:1}$, $\ref{eqn:2}$) and thus cannot be optimal. To see this, consider the joint log likelihood function of the system ($\ref{eqn:1}$, $\ref{eqn:2}$)
\begin{equation}
\begin{aligned} L(\beta, \rho, \alpha, \gamma)=&-\frac{1}{1-\delta^{2}} \sum_{t=1}^{T}\left[\frac{\left(r_{t}-\alpha-\beta x_{t-1}\right)^{2}}{\sigma_{u}^{2}}-2 \delta \frac{\left(r_{t}-\alpha-\beta x_{t-1}\right)\left(x_{t}-\gamma-\rho x_{t-1}\right)}{\sigma_{u} \sigma_{e}}\right.\\ &\left.+\frac{\left(x_{t}-\gamma-\rho x_{t-1}\right)^{2}}{\sigma_{e}^{2}}\right] \end{aligned}
\end{equation}
and observe that the t-test squared is equal to the likelihood ratio test statistic
\begin{equation}
\max _{\beta, \rho, \alpha, \gamma} L(\beta, \rho, \alpha, \gamma)-\max _{\rho, \alpha, \gamma} L\left(\beta_{0}, \rho, \alpha, \gamma\right)=t\left(\beta_{0}\right)^{2}.
\end{equation}
But this test statistic turns out to be the same if only the marginal log likelihood 
\begin{equation}
L(\beta, \alpha)=-\sum_{t=1}^{T}\left(r_{t}-\alpha-\beta x_{t-1}\right)^{2}
\end{equation}
is used in its computation. Thus, the t-test ignores information about $\rho$.

To reason about how to incorporate information about $\rho$ into the test, \citet{campbell2006efficient} assume $\rho$ to be known at first. If, further, the assumption $\alpha=\gamma=0$ is made, the only unknown variable that remains is $\beta$. Now, the likelihood function can be denoted as $L(\beta)$. If one restricts oneself to consider only the simple alternative of the form $\beta=\beta_{1}$ one can reason by the Neyman–Pearson Lemma that the most powerful test is of the form
\begin{equation}
\begin{aligned} \sigma_{u}^{2}\left(1-\delta^{2}\right)\left(L\left(\beta_{1}\right)-L\left(\beta_{0}\right)\right) = & 2\left(\beta_{1}-\beta_{0}\right) \sum_{t=1}^{T} x_{t-1}\left[r_{t}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)\right] \\ &-\left(\beta_{1}^{2}-\beta_{0}^{2}\right) \sum_{t=1}^{T} x_{t-1}^{2}>C, \end{aligned}
\end{equation}
with $\beta_{u e}=\sigma_{u e} / \sigma_{e}^{2}$ and $C$ being some constant.
This optimal test is not uniformly most powerful [UMP], however, since it is a weighted sum of minimal sufficient statistics and the weights depend on $\beta_{1}$. For that reason, \citet{campbell2006efficient} propose to condition the test on the ancillary statistic  $\sum_{t=1}^{T} x_{t-1}^{2}$. As a result the test can be simplified to
\begin{equation}
 \sum_{t=1}^{T} x_{t-1}\left[r_{t}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)\right]>C,
\end{equation}
which is UMP for alternatives of the form $\beta_{1}>\beta_{0}$ when $\rho$ is known.

For the test-statistic to have a standard normal distribution under the null, \citet{campbell2006efficient} propose to recenter and rescale it, which results in 
\begin{equation}
\frac{\sum_{t=1}^{T} x_{t-1}\left[r_{t}-\beta_{0} x_{t-1}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)\right]}{\sigma_{u}\left(1-\delta^{2}\right)^{1 / 2}\left(\sum_{t=1}^{T} x_{t-1}^{2}\right)^{1 / 2}}>C.
\end{equation}
Revoking the previous assumption of $\alpha=\gamma=0$ and generalizing instead to any unknown  $\alpha$ and $\gamma$ by replacing $x_{t-1}$ by its demeaned value $x_{t-1}^{\mu}$, the test
\begin{equation}
Q\left(\beta_{0}, \rho\right)=\frac{\sum_{t=1}^{T} x_{t-1}^{\mu}\left[r_{t}-\beta_{0} x_{t-1}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)\right]}{\sigma_{u}\left(1-\delta^{2}\right)^{1 / 2}\left(\sum_{t=1}^{T} x_{t-1}^{\mu 2}\right)^{1 / 2}}>C
\end{equation}
is UMP conditional on $\sum_{t=1}^{T} x_{t-1}^{\mu 2}$.
This Q-statistic, as \citet{campbell2006efficient} call it, has an intuitive interpretations when $\beta_{0} = 0$. It is the t-statistic of the $\beta^{\star}$ coefficient in the regression
 \begin{equation}
 \label{eqn:r_star}
r_{t}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)=\alpha^{\star}+\beta^{\star} x_{t-1}+v_{t}. 
\end{equation}
Regression ($\ref{eqn:r_star}$) can be interpreted as regressing the denoised returns onto the regressor $x_{t-1}$, where the information contained in $\rho$ and the correlation of the shocks is exploited.
When $\beta_{u e}=0$, i.e., the correlation of the shocks is zero, the Q-statistic simplifies to the t-statistic which converges to a standard-normal distribution in this case. When $\beta_{u e}\neq0$, the t-statistic does not converge to a standard normal distribution. Instead, as shown by \citet{elliott1994inference}, under local-to-unity asymptotic theory it has the null distribution 
\begin{equation}
\label{lua_t}
t\left(\beta_{0}\right) \Rightarrow \delta \frac{\tau_{c}}{\kappa_{c}}+\left(1-\delta^{2}\right)^{1 / 2} Z,
\end{equation}
where $\left(W_{u}(s), W_{e}(s)\right)^{\prime}$ is a two-dimensional Wiener process with correlation $\delta$, $J_{c}(s)$ is defined by $\mathrm{d} J_{c}(s)=c J_{c}(s) \mathrm{d} s+\mathrm{d} W_{e}(s)$ with $J_{c}(0)=0$, $Z$ is a standard normal random variable independent of $\left(W_{e}(s), J_{c}(s)\right)$, $\kappa_{c}=\left(\int J_{c}^{\mu}(s)^{2} \mathrm{d} s\right)^{1 / 2}$, and $\tau_{c}=\int J_{c}^{\mu}(s) \mathrm{d} W_{e}(s)$.

Within the local-to-unity asymptotic theory the persistence of the process $x_{t}$ is modeled as $\rho=1+c / T$ where $c$ is a constant and $T$ is the sample size. Hence, when $c<0$, $x_{t}$ is $I(0)$ but highly persistent and its sample moments do not converge in probability to constants but to functionals of a diffusion process instead. The t-test is not feasible since it depends on the unknown parameter $\rho$ through $\tau_{c} / \kappa_{c}$, which also makes it non-standard. 

\subsection{A pretest}
\label{pretest}
From Equation ($\ref{lua_t}$) it's easy to see that if $\delta = 0$, the nuisance term $\delta \frac{\tau_{c}}{\kappa_{c}}$ vanishes and the t-statistic collapses to the standard normal random variable $Z$. It therefore makes sense to test whether $\delta$ is different from zero. If it isn't, one can infer that the t-statistic is approximately standard normal and inference based on it is valid. This, however, is not the only condition in which there is no size distortion. \citet{phillips1987towards} shows that if $x_{t}$ is not persistent, $\tau_{c} / \kappa_{c}$ converges to a different standard normal random variable $Z^{\star}$. Hence the t-statistic, as a sum of two independent standard normal random variables, is likewise standard normal. 

\citet{campbell2006efficient} use these facts as bases for a pretest. This test supposes that an actual size $\alpha^{\star}  \geq \alpha$ may be acceptable if its not much greater than the nominal size $\alpha$. It then tests whether $\alpha^{\star}$ is greater than a prespecified acceptable size. It would be tempting to use a unit root test and infer no size distortion if its null of nonstationarity can be rejected. \citet{elliott1994inference} show, however, that this does not guarantee valid inference via the t-statistic. Instead, \citet{campbell2006efficient} use a unit root test statistic to construct a confidence interval for $c$ by inverting its alternative distribution in the spirit of \citet{stock1991confidence}. $\delta$, on the other hand, can be consistently estimated from the residuals of regressions ($\ref{eqn:1}$, $\ref{eqn:2}$). The size of the t-statistic forms a two-dimensional surface in the $c$-$\delta$-parameter space. If the confidence interval for $c$ lies strictly outside the region where the size is above the acceptable threshold given $\hat{\delta}$, the null of unacceptable size distortion can be rejected. This test is implemented with tables provided by \citet{campbell2006efficient}, who use the DF-GLS test statistic proposed by \citet{elliott1996efficient} to construct the confidence interval for $c$. The application accompanying this paper uses the closest tabulated values to the estimated values without interpolating.

\subsection{Making the Q-test feasible}
\label{Making the Q-test feasible}
When the pretest cannot reject the null of unacceptable size distortion, inference cannot be based on the t-test without adjustment. While the Q-test is UMP when $\rho$ and $\delta$ are known, in practice this is not the case. Furthermore, $\rho$ cannot be estimated consistently since its OLS estimator converges at rate $T$. Instead, analogously to the description in the previous section, \citet{campbell2006efficient} construct a $100\left(1-\alpha_{1}\right) \%$ confidence interval, $C_{\rho}\left(\alpha_{1}\right)$ for $\rho$ from a unit root test statistic. They then construct $C_{\beta | \rho}\left(\alpha_{2}\right)$, a $100\left(1-\alpha_{2}\right) \%$ conditional-on-$\rho$ confidence interval for $\beta$. To marginalize $\rho$ and thus getting an unconditional confidence interval for $\beta$ they take the union over $\rho \in C_{\rho}\left(\alpha_{1}\right)$
\begin{equation}
C_{\beta}(\alpha)=\bigcup_{\rho \in C_{\rho}\left(\alpha_{1}\right)} C_{\beta | \rho}\left(\alpha_{2}\right).
\end{equation}
$C_{\beta}(\alpha)$ has coverage of at least $100(1-\alpha) \%$ with $\alpha=\alpha_{1}+\alpha_{2}$. This follows from Bonferroni's inequality which gives this method its name.  
When the regressor is a valuation ratio, a negative value for $\delta$ should be expected. Further, note that $\beta(\rho)$ is given by 
\begin{equation}
\beta(\rho)=\frac{\sum_{t=1}^{T} x_{t-1}^{\mu}\left[r_{t}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)\right]}{\sum_{t=1}^{T} x_{t-1}^{\mu 2}}.
\end{equation}
Hence, the estimate of $\beta$ declines linearly in $\rho$ and its confidence interval is 
\begin{equation}
C_{\beta}(\alpha)=\left[\underline{\beta}\left(\overline{\rho}\left(\overline{\alpha}_{1}\right), \alpha_{2}\right), \overline{\beta}\left(\underline{\rho}\left(\underline{\alpha}_{1}\right), \alpha_{2}\right)\right],
\end{equation}
with $C_{\rho}\left(\alpha_{1}\right)= \left[\underline{\rho}\left(\underline{\alpha}_{1}\right), \overline{\rho}\left(\overline{\alpha}_{1}\right)\right]$ being the confidence interval for $\rho$, $\underline{\alpha}_{1}=\operatorname{Pr}\left(\rho<\underline{\rho}\left(\underline{\alpha}_{1}\right)\right)$, $\overline{\alpha}_{1}=\operatorname{Pr}\left(\rho>\overline{\rho}\left(\overline{\alpha}_{1}\right)\right)$, and $\alpha_{1}=\underline{\alpha}_{1}+\overline{\alpha}_{1}$. The Q-statistic is also known to be normally distributed. Let $z_{\alpha_{2} / 2}$ denote the $1-\alpha_{2} / 2$ quantile of the standard normal distribution. Then the lower bound is given by
\begin{equation}
\underline{\beta}\left(\overline{\rho}\left(\overline{\alpha}_{1}\right), \alpha_{2}\right)=\beta \left(\overline{\rho}\left(\overline{\alpha}_{1}\right) \right)-z_{\alpha_{2} / 2} \sigma_{u}\left(\frac{1-\delta^{2}}{\sum_{t=1}^{T} x_{t-1}^{\mu}}\right)^{1 / 2}
\end{equation}
and the upper bound is given by
\begin{equation}
\overline{\beta}\left(\underline{\rho}\left(\underline{\alpha}_{1}\right), \alpha_{2}\right)=\beta \left(\underline{\rho}\left(\underline{\alpha}_{1}\right) \right)+z_{\alpha_{2} / 2} \sigma_{u}\left(\frac{1-\delta^{2}}{\sum_{t=1}^{T} x_{t-1}^{\mu 2}}\right)^{1 / 2}.
\end{equation}
If this confidence interval does not contain zero, the regressor is inferred to have predictive power. While this test is valid, it is conservative, however, since Bonferroni's inequality is likely to be strict, i.e., the coverage of $C_{\beta}(\alpha)$ is likely greater than $100(1-\alpha) \%$. For that reason, \citet{campbell2006efficient} refine the confidence interval based on a numerical method proposed by \citet{cavaliere2005unit}. To obtain a tighter confidence interval with significance level $\widetilde{\alpha}$, according to this method, $\alpha_{2}$ is fixed first. $\operatorname{Pr}\left(\underline{\beta}\left(\overline{\rho}\left(\overline{\alpha}_{1}\right), \alpha_{2}\right)>\beta\right)$ is then evaluated for each $\delta$, and $\overline{\alpha}_{1}$ is selected such that 
\begin{equation}
\operatorname{Pr}\left(\underline{\beta}\left(\overline{\rho}\left(\overline{\alpha}_{1}\right),\alpha_{2}\right)>\beta\right) \leq \widetilde{\alpha} / 2\end{equation}
holds for all $c$ and, importantly,
\begin{equation}
\operatorname{Pr}\left(\underline{\beta}\left(\overline{\rho}\left(\overline{\alpha}_{1}\right), \alpha_{2}\right)>\beta\right) = \widetilde{\alpha} / 2
\end{equation}
holds for some $c$. 
Similarly, $\underline{\alpha}_{1}$ is selected such that 
\begin{equation}
\operatorname{Pr}\left(\overline{\beta}\left(\underline{\rho}\left(\underline{\alpha}_{1}\right), \alpha_{2}\right)<\beta\right) \leq \widetilde{\alpha} / 2
\end{equation}
holds again for all $c$ with equality at some $c$. A one-sided Bonferroni test based a confidence interval thus obtained has size $\widetilde{\alpha} / 2$ for some permissible $c$ while a two-sided test has at most size $\widetilde{\alpha}$.  

\subsection{Implementation of the feasible Q-test}
\label{implementation}

The feasible Q-test, generalized to less restrictive assumptions, is implemented in the R programming language of the \citet{R} following the steps proposed by \citet{campbell2005implementing}.\footnote{The source code is available at https://github.com/jpwoeltjen/PersistentRegressors/tree/master/pr.} Suppose the regressor follows an $AR(p)$ process. Further, the innovations are not necessarily normally distributed but are a martingale difference sequence, i.e., 
\begin{equation}
\mathrm{E}\left[w_{t} |\mathscr{F}_{t-1}\right]=0,
\end{equation}
where $\mathscr{F}_{t}=\left\{w_{s} | s \leq t\right\}$ denotes the filtration generated by $w_{t}=\left(u_{t}, e_{t}\right)^{\prime}$.
Its fourth moments are assumed to be finite and the unconditional variance is fixed, i.e.,
\begin{equation}
\label{eqn:covarst}
\begin{array}{l}{\mathrm{E}\left[w_{t} w_{t}^{\prime}\right]=\Sigma} \\ {\sup _{t} \mathrm{E}\left[u_{t}^{4}\right]<\infty, \sup _{t} \mathrm{E}\left[t_{t}^{4}\right]<\infty, \text { and } \mathrm{E}\left[x_{0}^{2}\right]<\infty.}\end{array}
\end{equation}
The dynamics of $x_{t}$ are written as
\begin{equation}
\label{eqn:22}
x_{t}=\gamma+\rho x_{t-1}+v_{t},
\end{equation}
where $\rho$ is the largest autoregressive root and $b(L) v_{t}=e_{t}$ with $b(L)=\sum_{i=0}^{p-1} b_{i} L^{i}$, $b_{0}=1$, and $b(1) \neq 0$. In this generalized case, the Q-statistic is not asymptotically standard normal distributed under the null. This is corrected by modifying it to
\begin{equation}
Q\left(\beta_{0}, \rho\right)=\frac{\sum_{t=1}^{T} x_{t-1}^{\mu}\left[r_{t}-\beta_{0} x_{t-1}-\sigma_{u e} /\left(\sigma_{e} \omega\right)\left(x_{t}-\rho x_{t-1}\right)\right]+\frac{T}{2} \sigma_{u e} /\left(\sigma_{e} \omega\right)\left(\omega^{2}-\sigma_{v}^{2}\right)}{\sigma_{u}\left(1-\delta^{2}\right)^{1 / 2}\left(\sum_{t=1}^{T} x_{t-1}^{\mu 2}\right)^{1 / 2}},
\end{equation}
where $\omega=\sigma_{e} / b(1)$.

Implementing the feasible Q-test, first $\hat{\beta}$ and its standard error $\operatorname{SE}(\widehat{\beta})$ are obtained by OLS estimation of Equation ($\ref{eqn:1}$). $x_{t}$ is written in its augmented Dickey-Fuller form
\begin{equation}
\Delta x_{t}=\tau+\theta x_{t-1}+\sum_{i=1}^{p-1} \psi_{i} \Delta x_{t-i}+e_{t},
\end{equation}
where $\psi_{i}=-\sum_{j=i}^{p-1} a_{j}$, $a(L)=L^{-1}[1-(1-\rho L) b(L)]$, and $\theta=(\rho-1) b(1)$. Estimation of $\widehat{\psi}_{i}(i=1, \ldots, p-1)$ is done via OLS where $p$ is either explicitly specified or estimated via the Bayesian information criterion [BIC].
With $\widehat{u}_{t}$ and $\widehat{e}_{t}$ being the residuals of regression (1) and (2), respectively,
\begin{align} \widehat{\sigma}_{u}^{2} &=\frac{1}{T-2} \sum_{t=1}^{T} \widehat{u}_{t}^{2} \\ \widehat{\sigma}_{e}^{2} &=\frac{1}{T-2} \sum_{t=1}^{T} \widehat{e}_{t}^{2} \\ \widehat{\sigma}_{u e} &=\frac{1}{T-2} \sum_{t=1}^{T} \widehat{u}_{t} \widehat{e}_{t} \\ \widehat{\delta} &=\frac{\widehat{\sigma}_{u e}}{\widehat{\sigma}_{u} \widehat{\sigma}_{e}} \\ \widehat{\omega}^{2}&=\widehat{\sigma}_{e}^{2} /\left(1-\sum_{i=1}^{p-1} \widehat{\psi}_{i}\right)^{2}
\end{align}
are then computed. OLS estimation of Equation ($\ref{eqn:22}$) yields $\widehat{\rho}$ and its standard error $\mathrm{SE}(\widehat{\rho})$. The residual $\widehat{v}_{t}$ is used to compute 
\begin{equation}
\widehat{\sigma}_{v}^{2}=(T-2)^{-1} \sum_{t=1}^{T} \widehat{v}_{t}^{2}.
\end{equation} 
Now, to compute the DF-GLS statistic, $\left(x_{0}, x_{1}-\rho_{G L S} x_{0}, \ldots, x_{T}-\rho_{G L S} x_{T-1}\right)^{\prime}$ is regressed onto 
$\left(1,1-\rho_{G L S}, \ldots, 1-\rho_{G L S}\right)^{\prime}$ with $\rho_{G L S}=1-7 / T$. The coefficient $\mu_{G L S}$ is used to compute $\overline{x}_{t}=x_{t}-\mu_{G L S}$ which, in turn, is needed for the regression 
\begin{equation}
\Delta \overline{x}_{t}=\theta \overline{x}_{t-1}+\sum_{i=1}^{p-1} \psi_{i} \Delta \overline{x}_{t-i}+e_{t},
\end{equation} which is estimated with no intercept. Finally, the t-statistic for $\theta$ is computed. This is the DF-GLS statistic. To confirm that no error is conducted in this estimation procedure, the DF-GLS statistic thus obtained is compared to the implementation in the urca package of \citet{urca}. 

The confidence interval $[\underline{\rho}, \overline{\rho}]=[1+\underline{c} / T, 1+\overline{c} / T]$ is obtained by first finding $[\underline{c}, \overline{c}]$ via tables 2–11 of \citet{campbell2005implementing}, where the closest values to the estimated DF-GLS statistic and $\hat{\delta}$ are used. Let a temporary variable $r^{\star}$ be defined by
\begin{equation}
r_{t}^{*}=r_{t}-\widehat{\sigma}_{u e} \widehat{\sigma}_{e}^{-2}\left(x_{t}-\rho x_{t-1}\right).
\end{equation}
Now the regression
\begin{equation}
r_{t}^{*}=\alpha+\beta x_{t-1}+u_{t}
\end{equation} 
is run for each $\rho=\{\underline{\rho}, \overline{\rho}\}$ to get $\widehat{\beta}(\rho)$. $[\underline{\beta}(\rho), \overline{\beta}(\rho)]$ is then obtained by computing

\begin{equation}
\underline{\beta}(\rho)=\widehat{\beta}(\rho)+\frac{T-2}{2} \frac{\widehat{\sigma_{ue}}}{\widehat{\sigma}_{e} \widehat{\omega}}\left(\frac{\widehat{\omega}^{2}}{\widehat{\sigma}_{v}^{2}}-1\right) \operatorname{SE}(\widehat{\rho})^{2}-1.645\left(1-\widehat{\delta}^{2}\right)^{1 / 2} \operatorname{SE}(\widehat{\beta})
\end{equation}
and 
\begin{equation}
\overline{\beta}(\rho)=\widehat{\beta}(\rho)+\frac{T-2}{2} \frac{\widehat{\sigma}_{u e}}{\widehat{\sigma}_{e} \widehat{\omega}}\left(\frac{\widehat{\omega}^{2}}{\widehat{\sigma}_{v}^{2}}-1\right) \operatorname{SE}(\widehat{\rho})^{2}+1.645\left(1-\widehat{\delta}^{2}\right)^{1 / 2} \operatorname{SE}(\widehat{\beta})
\end{equation}
A 5\% one-sided or 10\% two-sided test of $H_0: \beta = 0$ is based on the 90\% Bonferroni confidence interval
$$[\underline{\beta}(\overline{\rho}), \overline{\beta}(\underline{\rho})].$$
\subsection{Power comparison}
\label{power}
\citet{campbell2006efficient} evaluate the power of the feasible Q-test under local-to-unity asymptotics. In this framework, the OLS estimators $\hat{\beta}$ and $\hat{\rho}$ are consistent at rate $T$. Thus, to have a meaningful comparison, local alternatives of the form $\beta = \beta_0 + b/T$ are considered where $b$ is a constant. The feasible Q-test is found to dominate the feasible t-test in all cases considered with increasing relative power gain in increasing $|\delta|$ and decreasing $|c|$. Their analysis shows that in the case of a highly persistent regressor, using the Q-test over the t-test provides a comparatively important gain in power. Their numerical refinement of the Bonferroni method provides a substantial further gain in power for the Q-test since this test exploits information about $\rho$. This makes its confidence interval for $\beta$ given $\rho$ more sensitive to $\rho$. Hence, without the refinement the Bonferroni Q-test is too conservative.

\section{Monte Carlo simulation}
This section, next to the out-of-sample results in Section $\ref{Empirical Results}$, accommodates one of the main contributions of this paper.\footnote{All code generating the Monte Carlo results is available at https://github.com/jpwoeltjen/\\PersistentRegressors/tree/master/MonteCarlo.} First, Monte Carlo evidence of \citet{campbell2006efficient} is replicated providing reassurance that both parties implemented the method correctly. Next, the Bonferroni Q-test is tested by violating key assumptions about the regressor's persistence. Furthermore, innovations are modeled by increasingly realistic distributions, for which justification in the form of real-world parameter estimates is given.
\label{Monte Carlo simulation}
\subsection{Simulation with Gaussian innovations}
\label{Simulation with Gaussian innovations}
To confirm the correct implementation of the procedure, Table~\vref{tab:cy} replicates Table 3 of \citet{campbell2006efficient} with the same input parameters. For each parameter combination, 10,000 Monte Carlo samples are drawn from a multivariate standard normal distribution with correlation $\delta$. These variates represent the innovations, which are used to simulate sample paths according to system ($\ref{eqn:1}$, $\ref{eqn:2}$). The null hypothesis $H_0: \beta = 0$ is tested against the alternative $H_1: \beta > 0$ at the 5\% significance level. If the Bonferroni confidence interval lies strictly above zero, the one-sided test is rejected. 
\begin{table}[hb!]
\caption{Finite-sample rejection rates}
\label{tab:cy}
\centering

\begin{threeparttable}
\begin{tabular}{rrrrrrrr}
 \hline
 & Obs & $c$ & $\rho$ & $\delta$ & t-test & BQ & Q-test \\ 
  \hline
1 & 50 & 0 & 1.000 & -0.95 & 0.4160 & 0.0826 & 0.0483 \\ 
  2 & 50 & 0 & 1.000 & -0.75 & 0.2916 & 0.0837 & 0.0515 \\ 
  3 & 50 & -2 & 0.961 & -0.95 & 0.2714 & 0.0868 & 0.0482 \\ 
  4 & 50 & -2 & 0.961 & -0.75 & 0.2079 & 0.0881 & 0.0532 \\ 
  5 & 50 & -20 & 0.608 & -0.95 & 0.0977 & 0.1206 & 0.0515 \\ 
  6 & 50 & -20 & 0.608 & -0.75 & 0.0840 & 0.1078 & 0.0484 \\ 
  7 & 100 & 0 & 1.000 & -0.95 & 0.4217 & 0.0616 & 0.0480 \\ 
  8 & 100 & 0 & 1.000 & -0.75 & 0.2930 & 0.0616 & 0.0497 \\ 
  9 & 100 & -2 & 0.980 & -0.95 & 0.2698 & 0.0587 & 0.0505 \\ 
  10 & 100 & -2 & 0.980 & -0.75 & 0.2104 & 0.0588 & 0.0489 \\ 
  11 & 100 & -20 & 0.802 & -0.95 & 0.1063 & 0.0622 & 0.0471 \\ 
  12 & 100 & -20 & 0.802 & -0.75 & 0.0874 & 0.0514 & 0.0500 \\ 
  13 & 250 & 0 & 1.000 & -0.95 & 0.4259 & 0.0476 & 0.0483 \\ 
  14 & 250 & 0 & 1.000 & -0.75 & 0.2970 & 0.0506 & 0.0536 \\ 
  15 & 250 & -2 & 0.992 & -0.95 & 0.2866 & 0.0507 & 0.0481 \\ 
  16 & 250 & -2 & 0.992 & -0.75 & 0.2092 & 0.0466 & 0.0492 \\ 
  17 & 250 & -20 & 0.920 & -0.95 & 0.1080 & 0.0406 & 0.0517 \\ 
  18 & 250 & -20 & 0.920 & -0.75 & 0.0944 & 0.0369 & 0.0501 \\ 
   \hline
\end{tabular}
 \begin{tablenotes}
 \small
\item This Table displays the finite-sample rejection rates of the true null for the right-tailed Bonferroni Q-test [BQ] of predictability at $\alpha$=0.05. 10,000 Monte Carlo simulations are performed.
\end{tablenotes}
\end{threeparttable}
\end{table}

Next, slightly different input parameters are chosen. If the results differ qualitatively, the previous findings might be the result of selection bias. As demonstrated by Table~\vref{tab:cy_changed}, no such suspicion is warranted. As long as the sample size is at least 100 and $\rho$ is local-to-unity, the Bonferroni Q-test has acceptable finite sample rejection rates. The infeasible Q-test, unsurprisingly, always has rejection rates close to $\alpha$. The t-test, on the other hand, significantly over-rejects when the innovations are highly correlated and $\rho$ is local-to-unity. It should be noted, however, that the two-sided Bonferroni Q-test tends to under-reject for the chosen parameters. 
\begin{table}[h!]
\caption{Finite-sample rejection rates}
\centering
\label{tab:cy_changed}
\begin{threeparttable}
\begin{tabular}{rrrrrrrrr}
  \hline
 & Obs & $c$ & $\rho$ & $\delta$ & t-test & BQ& Q-test & BQ$_{{2sided}}$ \\ 
  \hline
1 & 50 & 0 & 1.000 & -0.99 & 0.4442 & 0.0863 & 0.0544 & 0.0864 \\ 
  2 & 50 & 0 & 1.000 & -0.90 & 0.3786 & 0.0943 & 0.0506 & 0.0953 \\ 
  3 & 50 & -1 & 0.980 & -0.99 & 0.3502 & 0.0857 & 0.0486 & 0.0898 \\ 
  4 & 50 & -1 & 0.980 & -0.90 & 0.3107 & 0.0949 & 0.0489 & 0.0964 \\ 
  5 & 50 & -10 & 0.804 & -0.99 & 0.1440 & 0.0821 & 0.0512 & 0.1004 \\ 
  6 & 50 & -10 & 0.804 & -0.90 & 0.1226 & 0.0840 & 0.0472 & 0.0929 \\ 
  7 & 50 & -15 & 0.706 & -0.99 & 0.1114 & 0.0895 & 0.0485 & 0.1112 \\ 
  8 & 50 & -15 & 0.706 & -0.90 & 0.1035 & 0.0981 & 0.0468 & 0.1112 \\ 
  9 & 150 & 0 & 1.000 & -0.99 & 0.4467 & 0.0508 & 0.0508 & 0.0509 \\ 
  10 & 150 & 0 & 1.000 & -0.90 & 0.3804 & 0.0524 & 0.0494 & 0.0554 \\ 
  11 & 150 & -1 & 0.993 & -0.99 & 0.3584 & 0.0475 & 0.0493 & 0.0611 \\ 
  12 & 150 & -1 & 0.993 & -0.90 & 0.3137 & 0.0572 & 0.0532 & 0.0625 \\ 
  13 & 150 & -10 & 0.934 & -0.99 & 0.1510 & 0.0468 & 0.0480 & 0.0720 \\ 
  14 & 150 & -10 & 0.934 & -0.90 & 0.1309 & 0.0534 & 0.0484 & 0.0672 \\ 
  15 & 150 & -15 & 0.901 & -0.99 & 0.1273 & 0.0446 & 0.0520 & 0.0819 \\ 
  16 & 150 & -15 & 0.901 & -0.90 & 0.1133 & 0.0484 & 0.0489 & 0.0695 \\ 
  17 & 200 & 0 & 1.000 & -0.99 & 0.4477 & 0.0435 & 0.0478 & 0.0438 \\ 
  18 & 200 & 0 & 1.000 & -0.90 & 0.3786 & 0.0501 & 0.0520 & 0.0546 \\ 
  19 & 200 & -1 & 0.995 & -0.99 & 0.3582 & 0.0449 & 0.0483 & 0.0607 \\ 
  20 & 200 & -1 & 0.995 & -0.90 & 0.3127 & 0.0542 & 0.0468 & 0.0615 \\ 
  21 & 200 & -10 & 0.950 & -0.99 & 0.1490 & 0.0401 & 0.0497 & 0.0666 \\ 
  22 & 200 & -10 & 0.950 & -0.90 & 0.1352 & 0.0486 & 0.0502 & 0.0634 \\ 
  23 & 200 & -15 & 0.925 & -0.99 & 0.1276 & 0.0369 & 0.0487 & 0.0786 \\ 
  24 & 200 & -15 & 0.925 & -0.90 & 0.1234 & 0.0482 & 0.0491 & 0.0670 \\ 
   \hline
\end{tabular}
 \begin{tablenotes}
 \small
\item This Table displays the finite-sample rejection rates of the true null for right-tailed and two-tailed Bonferroni Q-tests of predictability at $\alpha$=0.05 and $\alpha$=0.1, respectively. 10,000 Monte Carlo simulations are performed.
\end{tablenotes}
\end{threeparttable}
\end{table}
\subsection{Simulation with weakly persistent regressors and Gaussian innovations}
\label{Simulation with weakly persistent regressors and Gaussian innovations}
As argued by \citet{phillips2014confidence}, the feasible Q-test as derived in Section $\ref{Derivation of the tests}$ is asymptotically invalid when the largest autoregressive root of the regressor is not local-to-unity. In the following, finite-sample behavior is demonstrated as $\rho$ diverges from unity. Again, 10,000 Monte Carlo simulations are run with bivariate Gaussian innovations. A relatively small sample size is chosen to keep the DF-GLS statistic small in absolute value and thus stay in the vicinity of the implementation of \citet{campbell2005implementing}. Table~\vref{tab:non-local} shows that as $\rho$ distances itself from the unity neighborhood, the Bonferroni Q-test significantly over-rejects the true null of no predictability. Over-rejection is largest for highly correlated innovations and large $|c|$ where it surpasses 200\%.
\begin{table}[h!]
\caption{Finite-sample rejection rates (non local to unity autoregressive root) }
\label{tab:non-local}
\centering
\begin{threeparttable}
\begin{tabular}{rrrrrrrrr}
  \hline
 & Obs & $c$ & $\rho$ & $\delta$ & t-test & BQ & Q-test & BQ$_{{2sided}}$ \\ 
  \hline
1 & 50 & 0 & 1.000 & -0.95 & 0.4147 & 0.0946 & 0.0480 & 0.0957 \\ 
  2 & 50 & 0 & 1.000 & -0.75 & 0.2810 & 0.0838 & 0.0475 & 0.0870 \\ 
  3 & 50 & 0 & 1.000 & -0.50 & 0.1758 & 0.0846 & 0.0536 & 0.0940 \\ 
  4 & 50 & -5 & 0.902 & -0.95 & 0.1887 & 0.0896 & 0.0499 & 0.0980 \\ 
  5 & 50 & -5 & 0.902 & -0.75 & 0.1487 & 0.0830 & 0.0522 & 0.0891 \\ 
  6 & 50 & -5 & 0.902 & -0.50 & 0.1081 & 0.0701 & 0.0512 & 0.0811 \\ 
  7 & 50 & -10 & 0.804 & -0.95 & 0.1325 & 0.0894 & 0.0543 & 0.1018 \\ 
  8 & 50 & -10 & 0.804 & -0.75 & 0.1126 & 0.0789 & 0.0462 & 0.0867 \\ 
  9 & 50 & -10 & 0.804 & -0.50 & 0.0887 & 0.0689 & 0.0510 & 0.0820 \\ 
  10 & 50 & -20 & 0.608 & -0.95 & 0.0987 & 0.1266 & 0.0506 & 0.1469 \\ 
  11 & 50 & -20 & 0.608 & -0.75 & 0.0906 & 0.1087 & 0.0506 & 0.1216 \\ 
  12 & 50 & -20 & 0.608 & -0.50 & 0.0682 & 0.0800 & 0.0443 & 0.0948 \\ 
  13 & 50 & -30 & 0.412 & -0.95 & 0.0811 & 0.1666 & 0.0455 & 0.1961 \\ 
  14 & 50 & -30 & 0.412 & -0.75 & 0.0787 & 0.1272 & 0.0500 & 0.1460 \\ 
  15 & 50 & -30 & 0.412 & -0.50 & 0.0719 & 0.1003 & 0.0523 & 0.1195 \\ 
   \hline
\end{tabular}
 \begin{tablenotes}
 \small
\item This Table displays the finite-sample rejection rates of the true null for right-tailed  [BQ] and two-tailed  [BQ$_{{2sided}}$] Bonferroni Q-tests of predictability at $\alpha$=0.05 and $\alpha$=0.1, respectively, with non local to unity autoregressive root. 10,000 Monte Carlo simulations are performed.
\end{tablenotes}
\end{threeparttable}
\end{table}
\subsection{Simulation with CCC-GARCH innovations}
\label{Simulation with CCC-GARCH innovations}
\citet{campbell2006efficient} evaluate the robustness of the Q-test under a fat-tailed distribution. In particular, they find finite-sample rejection probabilities to be unchanged from the Gaussian case when the innovations follow a Student t-distribution with five degrees of freedom. As shown by \citet{cavaliere2005unit}, unit root tests may be invalid under conditional heteroskedasticity. Assumptions in the form of Equation ($\ref{eqn:covarst}$) imply validity of the Q-test under conditionally heteroskedastic innovations only if they are covariance stationary. 
It is therefore interesting to observe the behavior of the Bonferroni Q-test if innovations are modeled by a generalized autoregressive conditional heteroskedasticity [GARCH] process of \citet{bollerslev1986generalized} that approaches covariance nonstationarity. If the regressor is a valuation ratio, it seems sensible that there is a more or less stable correlation between the regressor and stock returns. A multivariate GARCH model that allows such a restriction is the Constant Conditional Correlations GARCH  [CCC-GARCH] model of \citet{bollerslev1990modelling}. It is defined as follows. Let $ \boldsymbol{z_{t}}$ be a bivariate standard normal random variable, i.e., $ \boldsymbol{z}_{t} \stackrel{i i d}{\sim} \mathrm{N}\left(\mathbf{0}, \boldsymbol{I}_{2}\right)$. Then the innovations are modeled as 
\begin{equation}
 \boldsymbol{w}_{t}=\boldsymbol{H}_{t}^{1 / 2} \boldsymbol{z}_{t},
\end{equation}
where $ \boldsymbol{H_t}$ denotes the covariance matrix at time $t$. $\boldsymbol{H}_{t}$ is a positive definite matrix whose Cholesky decomposition factors into 
\begin{equation}
\boldsymbol{H}_{t}^{1/2} = \boldsymbol{D}_{t} \boldsymbol{R}_{t}^{1 / 2},
\end{equation}
with $\boldsymbol{D}_{t}$ being a diagonal matrix with the conditional standard deviations 
\begin{equation}
\label{eqn:40}
\boldsymbol{D}_{t}=\left[\begin{array}{cc}{\sqrt{h_{1 t}^{2}}} & {0} \\ {0} & {\sqrt{h_{2 t}^{2}}}\end{array}\right]
\end{equation}
and $\boldsymbol{R}_{t}$ being the positive definite conditional correlation matrix 
\begin{equation}
\boldsymbol{R}_{t}=\left[\begin{array}{cc}{1} & {\delta_{ t}} \\ {\delta_{ t}} & {1}\end{array}\right].
\end{equation}
Since correlations are assumed to be constant through time, $\boldsymbol{R}_{t}=\boldsymbol{R}$. The conditional variances 
$${h}_{i t}^{2}, \quad t=1, \ldots, T, \quad i=1, 2$$
in $\boldsymbol{D}_{t}$ are modeled by independent univariate GARCH processes according to \citet{bollerslev1986generalized}, i.e.,
\begin{equation}
\label{eqn:garch}
\begin{aligned} \epsilon_{t} &=h_{t} \eta_{t}, \quad \eta_{t} \stackrel{i i d}{\sim}N(0,1) \\ h_{t}^{2} &=\omega+\sum_{i=1}^{q} \alpha_{i} \epsilon_{t-i}^{2}+\sum_{i=1}^{p} \beta_{i} h_{t-i}^{2} \\ \omega &>0, \quad \alpha_{i} \geq 0, \quad i=1, \ldots, q, \quad \beta_{i} \geq 0, \quad i=1, \ldots, p. \end{aligned}
\end{equation}
Simulation results are shown in Table~\vref{tab:ccc-garch}.
\begin{table}[ht]
\centering
\caption{Finite-sample rejection rates (CCC-GARCH(1, 1)) }
\label{tab:ccc-garch}
\begin{threeparttable}
\begin{tabular}{rrrrrrrrrrr}
  \hline
 & Obs & $c$ & $\rho$ & $\delta$ &$\hat{\delta}$ & $\alpha_{_{GARCH}}$ & $\beta_{_{GARCH}}$ & t-test &BQ& BQ$_{{2sided}}$ \\ 
  \hline
1 & 100 & -2 & 0.98 & -0.95 & -0.6002 & 0.0999 & 0.90 & 0.1819 & 0.0635 & 0.0697 \\ 
  2 & 100 & -2 & 0.98 & -0.95 & -0.6053 & 0.0999 & 0.80 & 0.1685 & 0.0633 & 0.0723 \\ 
  3 & 100 & -2 & 0.98 & -0.95 & -0.6080 & 0.0999 & 0.50 & 0.1677 & 0.0645 & 0.0744 \\ 
  4 & 100 & -2 & 0.98 & -0.95 & -0.6081 & 0.0999 & 0.40 & 0.1663 & 0.0596 & 0.0678 \\ 
  5 & 100 & -2 & 0.98 & -0.95 & -0.6077 & 0.0999 & 0.30 & 0.1616 & 0.0600 & 0.0696 \\ 
  6 & 100 & -2 & 0.98 & -0.95 & -0.6077 & 0.0999 & 0.20 & 0.1675 & 0.0610 & 0.0680 \\ 
  7 & 100 & -2 & 0.98 & -0.95 & -0.6086 & 0.0999 & 0.10 & 0.1741 & 0.0667 & 0.0742 \\ 
   \hline
\end{tabular}
 \begin{tablenotes}
 \small
\item This Table displays the finite-sample rejection rates of the true null for right-tailed  [BQ] and two-tailed  [BQ$_{{2sided}}$] Bonferroni Q-tests of predictability at $\alpha$=0.05 and $\alpha$=0.1, respectively, with innovations following a CCC-GARCH(1, 1) process. 10,000 Monte Carlo simulations are performed.
\end{tablenotes}
\end{threeparttable}
\end{table}
\subsection{Simulation with common GARCH innovations}
\label{Simulation with common GARCH innovations}
In the CCC-GARCH model it has been assumed that there are no volatility transmissions between the returns and the regressor. This assumption seems unrealistic and it reduces the correlation between the residuals as demonstrated by $\hat{\delta}$. There are various multivariate GARCH models suited for incorporating spill-over effects such as the BEKK model of \citet{engle1995multivariate}. It seems plausible, though, that the variances of both the regressor and the returns share a common stochastic trend which may be imagined as the overall economic uncertainty. This seems reasonable since it is hard to imagine having high variance in returns without having high variance in valuation ratios and vice versa. Parameter estimates of the univariate GARCH model based on monthly returns from 1926 to 2003 are displayed in Table~\vref{tab:garch_est}. Since $\hat{\alpha}+\hat{\beta}=0.983$ is close to unity, at which point the covariance stationarity condition is violated, concern of conditional heteroskedasticity of real returns leading to invalid inference using the feasible Q-test is justified.
\begin{table}[!htbp] \centering 
  \caption{Parameter Estimates of the GARCH(1, 1)} 
  \label{tab:garch_est} 
  \begin{threeparttable}
\begin{tabular}{@{\extracolsep{5pt}} ccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 &  Estimate &  Rob. Std. Error &  Rob. t value & Pr(\textgreater \textbar t\textbar ) \\ 
\hline \\[-1.8ex] 
$\omega$ & $0.0001$ & $0.00002$ & $2.553$ & $0.011$ \\ 
$\alpha$ & $0.107$ & $0.020$ & $5.331$ & $0.000$ \\ 
$\beta$ & $0.876$ & $0.020$ & $44.365$ & $0.000$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
 \begin{tablenotes}
 \small
\item Data is taken from Motohiro Yogo's website. Returns are the CRSP value weighted index log-returns from 1926 to 2003 minus the risk free rate.
\end{tablenotes}
\end{threeparttable}
\end{table}

Simulation is performed similar as in the CCC-GARCH model but now $h_{1, t} =h_{2, t}$ in Equation ($\ref{eqn:40}$). Results in Table~\vref{tab:cst-garch} compared with Table~\vref{tab:cy}  indicate that the one-sided Bonferroni Q-test tends to over-reject the true null if innovations are modeled as described above.
\begin{table}[hbt]
\centering
\caption{Finite-sample rejection rates (common stochastic trend)}
\label{tab:cst-garch}
\begin{threeparttable}
\begin{tabular}{rrrrrrrrrr}
  \hline
  & Obs & $c$ & $\rho$ & $\delta$ & $\alpha_{_{GARCH}}$ & $\beta_{_{GARCH}}$ & t-test &BQ& BQ$_{{2sided}}$ \\ 
  \hline
1 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.90 & 0.2815 & 0.0952 & 0.0990 \\ 
  2 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.80 & 0.2762 & 0.1009 & 0.1048 \\ 
  3 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.50 & 0.2706 & 0.0942 & 0.0977 \\ 
  4 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.40 & 0.2676 & 0.0961 & 0.1004 \\ 
  5 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.30 & 0.2628 & 0.0932 & 0.0972 \\ 
  6 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.20 & 0.2704 & 0.0908 & 0.0943 \\ 
  7 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.10 & 0.2677 & 0.0935 & 0.0984 \\ 
  8 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.90 & 0.3135 & 0.0733 & 0.0771 \\ 
  9 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.80 & 0.2812 & 0.0703 & 0.0771 \\ 
  10 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.50 & 0.2780 & 0.0683 & 0.0747 \\ 
  11 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.40 & 0.2797 & 0.0676 & 0.0742 \\ 
  12 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.30 & 0.2758 & 0.0664 & 0.0734 \\ 
  13 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.20 & 0.2746 & 0.0629 & 0.0703 \\ 
  14 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.10 & 0.2749 & 0.0656 & 0.0715 \\ 
   \hline
\end{tabular}
 \begin{tablenotes}
 \small
\item This Table displays the finite-sample rejection rates of the true null for right-tailed [BQ] and two-tailed  [BQ$_{{2sided}}$] Bonferroni Q-tests of predictability at $\alpha$=0.05 and $\alpha$=0.1, respectively, with the variances of innovations sharing a common stochastic trend which is modeled as a univariate GARCH(1, 1) process. 10,000 Monte Carlo simulations are performed.
\end{tablenotes}
\end{threeparttable}
\end{table}

\subsection{Simulation with common GJR-GARCH innovations}
\label{Simulation with common GJR-GARCH innovations}
\citet*{glosten1993relation} [GJR] observe that in financial markets, negative shocks contribute disproportionately more to future return volatility than positive shocks. To model the dynamics of real markets more adequately, innovation variances are modeled by the conditional variance of a common univariate GJR-GARCH process. This process models conditional variance as
\begin{equation}
h_{t}^{2}=\omega+\left(\alpha+\gamma S_{t-1}\right) \epsilon_{t-1}^{2}+\beta h_{t-1}^{2},
\end{equation}
where $$S_{t-1}=\left\{\begin{array}{ll}{1} & {\text { if } \epsilon_{t-1}<0} \\ {0} & {\text { if } \epsilon_{t-1} \geq 0.}\end{array}\right.$$
When $\gamma>0$, negative shocks increase volatility more than positive shocks. The GJR-GARCH process is covariance stationary if $\alpha+\gamma / 2+\beta<1$. Further, to allow for conditional leptokurtosis, $\eta_{t}$ in Equation ($\ref{eqn:garch}$) now follows a generalized error distribution with parameter $\theta$, i.e., $$\eta_{t} \stackrel{i i d}{\sim}GED(\theta).$$ Parameter estimates for monthly returns of the CRSP value weighted index from 1926 to 2003 are displayed in Table~\vref{tab:gjr-garch}. Since $\hat{\alpha}+\hat{\gamma} / 2+\hat{\beta}=0.958$ real return volatilities are close to nonstationary.
\begin{table}[!htbp] \centering 
  \caption{Parameter Estimates of the GJR-GARCH(1, 1)} 
  \label{tab:gjr-garch} 
  \begin{threeparttable}
\begin{tabular}{@{\extracolsep{5pt}} ccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 &  Estimate &  Rob. Std. Error &  Rob. t value & Pr(\textgreater \textbar t\textbar ) \\ 
\hline \\[-1.8ex] 
$\omega$ & $0.0001$ & $0.00004$ & $2.982$ & $0.003$ \\ 
$\alpha$ & $0.040$ & $0.025$ & $1.599$ & $0.110$ \\ 
$\beta$ & $0.861$ & $0.028$ & $30.881$ & $0.000$ \\ 
$\gamma$ & $0.114$ & $0.045$ & $2.562$ & $0.010$ \\ 
$\theta$ & $1.515$ & $0.089$ & $17.058$ & $0.000$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
 \begin{tablenotes}
 \small
\item Data is taken from Motohiro Yogo's website. Returns are the CRSP value weighted index log-returns from 1926 to 2003 minus the risk free rate.
\end{tablenotes}
\end{threeparttable}
\end{table}
Monte Carlo paths are simulated with $\omega = 0.000118$, $\gamma=0.0999$, $\theta=1.5$, $\alpha=0$ and letting $\beta$ increase to 0.95 such that the process approaches nonstationarity. The simulation outcome is displayed in Table~\vref{tab:mc_grj-garch}. It is observed that under realistic volatility modeling, the right-tailed Bonferroni Q-test tends to over-reject the true null of no predictability. Even for the larger sample size of 100, the amount of over-rejection of the right-tailed Q-test at 5\% significance level surpasses 50\%. The null for the smaller sample size is over-rejected more than 100\%. For very large sample sizes of more than 1000 observations, a single spike of over-rejection is found when the GARCH process is very near nonstationarity. Otherwise, rejection rates are close to the nominal size.

\begin{table}[h!]
\centering
\caption{Finite-sample rejection rates (common stochastic trend GJR-GARCH)}
\label{tab:mc_grj-garch}
\begin{threeparttable}
\begin{tabular}{rrrrrrrrrr}
  \hline
  & Obs & $c$ & $\rho$ & $\delta$ & $\alpha_{_{GARCH}}$ & $\beta_{_{GARCH}}$ & t-test &BQ& BQ$_{{2sided}}$ \\ 
  \hline
1 & 50 & -2 & 0.961 & -0.9500 & 0.0000 & 0.95 & 0.2836 & 0.1001 & 0.1033 \\ 
  2 & 50 & -2 & 0.961 & -0.9500 & 0.0000 & 0.90 & 0.2736 & 0.0946 & 0.0986 \\ 
  3 & 50 & -2 & 0.961 & -0.9500 & 0.0000 & 0.80 & 0.2635 & 0.0952 & 0.0993 \\ 
  4 & 100 & -2 & 0.980 & -0.9500 & 0.0000 & 0.95 & 0.3054 & 0.0757 & 0.0802 \\ 
  5 & 100 & -2 & 0.980 & -0.9500 & 0.0000 & 0.90 & 0.2811 & 0.0662 & 0.0727 \\ 
  6 & 100 & -2 & 0.980 & -0.9500 & 0.0000 & 0.80 & 0.2801 & 0.0685 & 0.0734 \\ 
  7&1000 & -2 & 0.998 & -0.9500 & 0.0000 & 0.95 & 0.4619 & 0.1065 & 0.1124 \\ 
   \hline
\end{tabular}
 \begin{tablenotes}
 \small
\item This Table displays the finite-sample rejection rates of the true null for right-tailed [BQ] and two-tailed  [BQ$_{{2sided}}$] Bonferroni Q-tests of predictability at $\alpha$=0.05 and $\alpha$=0.1, respectively, with the variances of innovations sharing a common stochastic trend which is modeled as a univariate GJR-GARCH(1, 1) process. 10,000 Monte Carlo simulations are performed.
\end{tablenotes}
\end{threeparttable}
\end{table}


\section{Empirical results}
\label{Empirical Results}
\subsection{Replication of empirical results from \citet{campbell2006efficient}}
Table~\vref{tab:cy_sp} replicates the key results of Tables 4 and 5 of \citet{campbell2006efficient}. The Bonferroni Q-test is implemented as described in Section $\ref{implementation}$. The regressors under consideration are the dividend-to-price ratio [ldp] and the earnings-to-price ratio [lep], where earnings, following \citet{shiller2000irrational}, are averaged over a moving 10-year period. Three different sampling frequencies, annual, quarterly, and monthly, are considered. Following \citet{campbell2006efficient}, the confidence interval for $\beta$ is scaled by $\widehat{\sigma}_{e} / \widehat{\sigma}_{u}$ such that interpretation is made easier as the coefficient in Equation ($\ref{eqn:1}$) with unit variance innovations. Right-tailed tests with null hypothesis $H_0: \beta=0$ against alternative $H_1: \beta>0$ are considered, where the null is rejected at 5\% significance level if the lower bound of $\beta$'s 90\% confidence interval is greater than zero. Apart from rounding errors, the results are nearly the same except for the following discrepancies. First, the DF-GLS statistic for $p>1$ is higher than reported by \citet{campbell2006efficient}.  As previously mentioned, the DF-GLS implementation of Section $\ref{implementation}$ was verified with a different open-source implementation. Second, based on the BIC, a lower $p$ of 1 was selected for monthly ldp. Investigations showed that the difference in BIC when $p=1$ compared with $p=2$ is very small. Thus the different selection is likely due to rounding errors. 
\begin{table}[h!]
\small
\setlength\tabcolsep{5.0pt}
\centering
\caption{Replicated estimates of model parameters from \citet{campbell2006efficient}}
\label{tab:cy_sp}
\begin{threeparttable}
\begin{tabular}{llrrlrrrrrl}
  \hline
dataset & $x$ & obs & $\hat{\delta}$ & $CI_{\rho}$ & DF-GLS & $p$ &t-stat & pt & $\hat{\beta}$ & $CI_{\beta}$ \\ 
  \hline
  SP\_A & lep & 123 & -0.96 & [0.786,0.931] & -2.888 & 1 & 2.76 & 0 & 0.127 & [0.043,0.225] \\ 
  SP\_A & ldp & 123 & -0.85 & [0.940,1.006] & -1.247 & 3 & 1.95 & 0 & 0.083 & [-0.024,0.136] \\ 
  CRSP\_A & lep & 77 & -0.96 & [0.778,0.960] & -2.229 & 1 & 2.77 & 0 & 0.162 & [0.040,0.273] \\ 
  CRSP\_A & ldp & 77 & -0.72 & [0.926,1.010] & -1.033 & 1 & 2.53 & 0 & 0.158 & [0.013,0.186] \\ 
  CRSP\_Q & lep & 305 & -0.99 & [0.944,0.992] & -2.191 & 1 & 2.91 & 0 & 0.047 & [0.011,0.066] \\ 
  CRSP\_Q & ldp & 305 & -0.94 & [0.962,0.999] & -1.696 & 1 & 2.06 & 0 & 0.034 & [-0.009,0.044] \\ 
  CRSP\_M & lep & 913 & -0.99 & [0.985,1.000] & -1.859 & 1 & 2.66 & 0 & 0.013 & [0.001,0.018] \\ 
  CRSP\_M & ldp & 913 & -0.95 & [0.990,1.001] & -1.433 & 1 & 1.70 & 0 & 0.008 & [-0.005,0.010] \\ 
\hline
\end{tabular}
 \begin{tablenotes}
 \small
\item Data is taken from Motohiro Yogo's website at https://drive.google.com/file/d/0BzR-ojpYuaFMcnZteHFyWUVIUFU/view. The code generating this table is available at https://github.com/jpwoeltjen/PersistentRegressors. The regressors are the 10-year moving average earnings to current price ratio [lep] and the dividend to price ratio [ldp] in logs. Observations are recorded on a monthly [CRSP M], quarterly [CRSP Q], and annual basis [CRSP A] for CRSP (1926– 2002). For S\&P (1880– 2002) data is only available on an annual basis [SP A]. Stock returns are the SP 500 value weighted index log-returns from 1880 to 2003 minus the risk free rate and the CRSP value weighted index log-returns from 1926 to 2003 minus the risk free rate. $p$ denotes the optimal number of lags selected by BIC. pt is a boolean denoting whether the pretest of Section $\ref{pretest}$ rejects the null of actual size greater than 0.075. $CI_{\rho}$ and $CI_{\beta}$ denote the confidence interval for $\rho$ and $\beta$, respectively.
\end{tablenotes}
\end{threeparttable}
\end{table}
For all datasets, both regressors are highly negatively correlated with returns. The confidence interval for $\rho$ is very near to unity in all cases, and containing it in some. Hence, the pretest of Section $\ref{pretest}$ indicates non-standard null distribution of the t-test for every sampling frequency and regressor, invalidating inference based on t-statistics, which otherwise would be statistically significant. Based on the Bonferroni Q-test, the null hypothesis of no predictability can be rejected at the 5\% significance level for ldp and lep at the annual sampling frequency and for all datasets, respectively. 
\subsection{Out-of-sample results}
Based on an extended dataset taken from Amid Goyal's website, Table~\vref{tab:extended} shows that only lep on a quarterly frequency can be inferred to be predictive of excess log-returns. This discrepancy may arise from two causes. First, returns in the extended dataset are of the S\&P500 instead of the CRSP value-weighted index. Also, due to the lack of availability of the 1-month T-bill rate, all risk free-rate computations are based on the 3-month T-bill in the extended dataset. This introduces minor differences in datasets since \citet{campbell2006efficient} use the 1-month T-bill as a risk-free proxy for their monthly regressions. Second, as will be seen in Table~\vref{tab:oos}, predictability seems to have been weakening in recent years. Hence, a time-extended sample will show less evidence of it.  
\begin{table}[h!]
\small
\setlength\tabcolsep{5.0pt}
\centering
\caption{Replicated estimates of model parameters from \citet{campbell2006efficient} with an extended dataset}
\label{tab:extended}
\begin{threeparttable}
\begin{tabular}{llrrlrrrrrl}
%\begin{tabular}{ccccccccccc}
  \hline
dataset & $x$ & obs & $\hat{\delta}$ & $CI_{\rho}$ & DF-GLS & $p$ &t-stat & pt & $\hat{\beta}$ & $CI_{\beta}$ \\ 
  \hline
    CRSP\_A  & lep & 92 & -0.97 & [0.827,0.979] & -2.092 & 1 & 2.12 & 0 & 0.114 & [-0.01,0.180] \\ 
    CRSP\_A  & ldp & 92 & -0.86 & [0.875,0.986] & -1.731 & 1 & 0.97 & 0 & 0.042 & [-0.069,0.107] \\ 
    %CRSP\_A  & tbl & 66 & 0.08 & [0.853,0.908] & -1.899 & 1 & -0.41 & 1 & -0.279 & [-0.125,0.075] \\ 
    %CRSP\_A  & tms & 66 & -0.06 & [0.454,0.575] & -3.942 & 1 & 0.96 & 1 & 1.297 & [-0.071,0.275] \\ 
   CRSP\_Q & lep & 365 & -0.98 & [0.973,1.002] & -1.515 & 1 & 3.16 & 0 & 0.048 & [0.001,0.039] \\ 
  CRSP\_Q & ldp & 365 & -0.95 & [0.976,1.002] & -1.386 & 1 & 1.82 & 0 & 0.023 & [-0.012,0.026] \\ 
  %CRSP\_Q & tbl & 261 & -0.09 & [0.956,0.975] & -1.985 & 1 & -0.64 & 1 & -0.099 & [-0.045,0.019] \\ 
  %CRSP\_Q& tms & 261 & 0.06 & [0.835,0.864] & -4.392 & 1 & 1.44 & 1 & 0.489 & [-0.005,0.104] \\ 
  CRSP\_M & lep & 1104 & -0.99 & [0.993,1.002] & -1.343 & 1 & 2.40 & 0 & 0.010 & [-0.002,0.009] \\ 
  CRSP\_M & ldp & 1104 & -0.98 & [0.993,1.002] & -1.221 & 1 & 1.20 & 0 & 0.004 & [-0.005,0.006] \\ 
  %CRSP\_M & tbl & 792 & -0.13 & [0.992,0.997] & -1.428 & 1 & -0.91 & 1 & -0.043 & [-0.013,0.003] \\ 
  %CRSP\_M & tms & 792 & 0.04 & [0.958,0.967] & -3.831 & 1 & 1.49 & 1 & 0.156 & [-0.001,0.032] \\ 
\hline
\end{tabular}
 \begin{tablenotes}
 \small
\item Data is taken from Amit Goyal’s Website. The code generating this table is available at https://github.com/jpwoeltjen/PersistentRegressors. Stock returns are the SP 500 index log-returns from 1926 to 2017 from the Center for Research in Security Press (CRSP) minus the rolled over 3-month T-bill rate. lep is the log 10 year moving average earnings/price ratio (1926 to 2017). ldp is the log dividend/price ratio (1926 to 2017).
% tbl is the 3-month T-bill rate (1952 to 2017). tms is the term-spread between long-term government bonds and tbl (1952 to 2017). 
$p$ denotes the optimal number of lags selected by BIC. pt is a boolean denoting whether the pretest of Section $\ref{pretest}$ rejects the null of actual size greater than 0.075. $CI_{\rho}$ and $CI_{\beta}$ denote the confidence interval for $\rho$ and $\beta$, respectively.
\end{tablenotes}
\end{threeparttable}
\end{table}

Since the original publication of \citet{campbell2006efficient} a sufficiently large amount of data has been generated at the quarterly and monthly sampling frequencies that merits a standalone out-of-sample analysis. Table~\vref{tab:oos} displays model parameter estimates since 2004 until the last available data point in 2017. This relatively small dataset allows no rejection of the null hypothesis at the 5\% significance level for any of the regressors and sampling frequencies considered. Both regressors are still highly persistent at all sampling frequencies, demonstrated by the confidence interval for $\rho$ containing unity in all cases. 
\begin{table}[h!]
\small
\setlength\tabcolsep{5.0pt}
\centering
\caption{Out-of-sample estimates of model parameters}
\label{tab:oos}
\begin{threeparttable}
\begin{tabular}{llrrlrrrrrl}
  \hline
dataset & $x$ & obs & $\hat{\delta}$ & $CI_{\rho}$ & DF-GLS & $p$ &t-stat & pt & $\hat{\beta}$ & $CI_{\beta}$ \\ 
  \hline
  %CRSP\_A  & lep & 14 & -0.97 & [0.036,0.97] & -1.821 & 1 & 1.81 & 0 & 0.527 & [-0.055,1.07] \\ 
  %CRSP\_A  & ldp & 14 & -0.92 & [-1.468,0.015] & -3.283 & 1 & 1.63 & 0 & 0.446 & [0.319,2.044] \\ 
  %CRSP\_A  & tbl & 14 & -0.92 & [-1.468,0.015] & -3.283 & 1 & -1.47 & 1 & -3.865 & [0.319,2.044] \\ 
  %CRSP\_A  & tms & 14 & -0.01 & [0.333,0.593] & -1.791 & 1 & 0.67 & 1 & 2.630 & [-0.258,0.611] \\ 
  CRSP\_Q  & lep & 53 & -0.99 & [0.792,1.023] & -1.607 & 1 & 1.48 & 0 & 0.101 & [-0.059,0.202] \\ 
  CRSP\_Q & ldp & 53 & -0.97 & [0.778,1.001] & -1.708 & 1 & 0.86 & 0 & 0.060 & [-0.134,0.142] \\ 
 % CRSP\_Q  & tbl & 53 & -0.97 & [0.778,1.001] & -1.708 & 1 & -0.67 & 1 & -0.409 & [-0.134,0.142] \\ 
  %CRSP\_Q  & tms & 53 & 0.19 & [0.904,0.955] & -1.335 & 1 & -0.29 & 1 & -0.248 & [-0.1,0.077] \\ 
  CRSP\_M & lep & 168 & -0.99 & [0.955,1.014] & -1.211 & 1 & 1.02 & 0 & 0.020 & [-0.025,0.041] \\ 
  CRSP\_M & ldp & 168 & -0.98 & [0.961,1.012] & -1.091 & 1 & 0.59 & 0 & 0.012 & [-0.052,0.012] \\ 
  %CRSP\_M & tbl & 168 & 0.21 & [0.991,1.001] & -0.619 & 1 & -0.66 & 1 & -0.120 & [-0.016,0.006] \\ 
  %CRSP\_M & tms & 168 & 0.10 & [0.991,1.001] & -0.565 & 1 & -0.58 & 1 & -0.142 & [-0.035,0.02] \\ 
\hline
\end{tabular}
 \begin{tablenotes}
 \small
\item Data is taken from Amit Goyal’s Website. The code generating this table is available at https://github.com/jpwoeltjen/PersistentRegressors. Stock returns are the SP 500 index log-returns from 2004 to 2017 from the Center for Research in Security Press (CRSP) minus the rolled over 3-month T-bill rate. lep is the log 10 year moving average earnings/price ratio (2004 to 2017). ldp is the log dividend/price ratio (2004 to 2017).
% tbl is the 3-month T-bill rate (1952 to 2017). tms is the term-spread between long-term government bonds and tbl (1952 to 2017). 
$p$ denotes the optimal number of lags selected by BIC. pt is a boolean denoting whether the pretest of Section $\ref{pretest}$ rejects the null of actual size greater than 0.075. $CI_{\rho}$ and $CI_{\beta}$ denote the confidence interval for $\rho$ and $\beta$, respectively.
\end{tablenotes}
\end{threeparttable}
\end{table}

\section{Conclusion}
\label{conclusion}

The Bonferroni Q-test is an asymptotically valid test of predictability if the regressor's largest autoregressive root is local-to-unity and the innovations of regressor and regressand are bivariate Gaussian. It has been shown in the original work by \citet{campbell2006efficient} that the Bonferroni Q-test has important power advantages over the Bonferroni t-test. This study confirms acceptable finite-sample rejection rates when innovations are Gaussian and the assumptions of the Bonferroni Q-test are fulfilled. When the regressor is nonlocal-to-unity, however, the Q-test can over-reject the null in finite samples. Further, if innovations are modeled by a covariance nonstationarity approaching GARCH process, the right-tailed test again tends to over-reject the null of no predictability. The practical importance of this effect is demonstrated by GARCH parameter estimates based on S\&P 500 returns. Previous evidence of predictability of the earnings-to-price ratio and dividend-to-price ratio has been replicated. Some doubt, however, is casted by Monte Carlo evidence that finds over-rejection of the null for typical variance dynamics of real markets. Out-of-sample evidence, based on data emerged since the first publication of the method, is eroding. In particular, both regressors are not found statistically significant at the 5\% level at the monthly as well as the quarterly sampling frequency. In all cases, the pretest cannot reject the null of unacceptable size distortion of the t-test. Hence, inference based on the unadjusted t-test is invalid.

\pagebreak
\bibliographystyle{dcu}
\bibliography{pr.bib}

\end{document}
