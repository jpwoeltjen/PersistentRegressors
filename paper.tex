
\documentclass{article}
\usepackage{amsmath}
\usepackage{mathrsfs}  
\usepackage{float}
\usepackage{varioref}
\usepackage[margin=1.5in]{geometry}
\usepackage[flushleft]{threeparttable}
\title{Predictive Regressions with Persistent Regressors}
\author{Jan Philipp Wöltjen  \\
	Seminar in Statistics CAU Kiel  \\
	}

\date{\today}
% Hint: \title{what ever}, \author{who care} and \date{when ever} could stand 
% before or after the \begin{document} command 
% BUT the \maketitle command MUST come AFTER the \begin{document} command! 
\begin{document}

\maketitle


\begin{abstract}
Campbell and Yogo (2006) propose a Bonferroni-type procedure for valid inference of predictive regressions with almost non-stationary regressors. In this study, I will explain the rationale behind their method, replicate empirical findings, and extend them with out-of-sample data that surfaced since their method's publication. Further, I will investigate via Monte Carlo simulation the method's behavior when its assumptions are violated in ways likely encountered in practice. 
\end{abstract}

\section{Introduction}
The question whether asset returns can be predicted with the help of observable variables is of great interest to financial institutions and the public as a whole. One of the most intuitive relationships exists between the price of a company's stock and the discounted earnings it will generate in the future. Since future earnings are not observable, current earnings or dividends or moving averages of them are often used as a proxy. To statistically test whether these variables can indeed predict future stock returns one may fit a linear model of the form 
\begin{align} 
r_{t} = \alpha + \beta x_{t-1} + u_{t} \\ 
x_{t} = \gamma + \rho  x_{t-1} + e_{t}
\end{align}
and perform a t-test on the $\beta$ coefficient. 

\paragraph{Outline}

\section{The regression setup and its problem
}
With log-returns $r_{t}$ observable at time $t$ and lagged supposedly predictive variable $x_{t-1}$ observable at $t-1$  Campbell and Yogo (2006) consider the regression system
\begin{align} 
\label{eqn:1}
r_{t} = \alpha + \beta x_{t-1} + u_{t} \\ 
\label{eqn:2}
x_{t} = \gamma + \rho  x_{t-1} + e_{t}
\end{align}
They further assume normality i.e. $w_{t}=\left(u_{t}, e_{t}\right)^{\prime} \stackrel{i i d}{\sim} \mathrm{N}(\boldsymbol{0}, \boldsymbol{\Sigma})$ where  $\boldsymbol{\Sigma}=\left[\begin{array}{cc}{\sigma_{u}^{2}} & {\sigma_{u e}} \\ {\sigma_{u e}} & {\sigma_{e}^{2}}\end{array}\right]$ is known .


Whether $x_{t-1}$ is indeed predictive may be ascertained by evaluating the statistically significance of the $\beta$ coefficient obtained from an ordinary least squares [OLS] regression i.e. testing the null $H_0: \beta = 0$. A common approach is to perform a t-test of $\beta$. Elliott and Stock (1994), however, show that the t-test leads to invalid inference if $x_{t}$ is persistent i.e. if $\rho$ is close to unity and the noise terms $u_{t}$ and $e_{t}$ are highly correlated. Since the regressor is a valuation ratio which is a function of the stock price and the regressant is the stock's return which is also a function of the current price we have reason to believe that $u_{t}$ and $e_{t}$ will indeed be highly (negatively) correlated. Section ... will confirm this belief by empirical sample moments. It will also be seen that the regressor is highly persistent. Hence, we have to worry about the validity of the t-test. A pretest that tests the null of the actual size of the t-test being greater than some deemed-acceptable nominal size $\alpha$ and is based on the joint presence of both aforementioned conditions is explained in section ....

\section{Derivation of the tests}
\subsection{Infeasible tests}
In developing their test, Campbell and Yogo (2006) start by considering the t-statistic 
\begin{equation}
t\left(\beta_{0}\right)=\frac{\widehat{\beta}-\beta_{0}}{\sigma_{u}\left(\sum_{t=1}^{T} x_{t-1}^{\mu 2}\right)^{-1 / 2}}
\end{equation}
where $x_{t-1}^{\mu}=x_{t-1}-T^{-1} \sum_{t=1}^{T} x_{t-1}$ denotes the de-meaned regressor and $\widehat{\beta}$ is the OLS estimate of $\beta$. They argue that this test ignores information contained in the system (2) and can thus not be optimal. To see this consider the joint log likelihood function of the system ($\ref{eqn:1}$, $\ref{eqn:2}$)
\begin{equation}
\begin{aligned} L(\beta, \rho, \alpha, \gamma)=&-\frac{1}{1-\delta^{2}} \sum_{t=1}^{T}\left[\frac{\left(r_{t}-\alpha-\beta x_{t-1}\right)^{2}}{\sigma_{u}^{2}}-2 \delta \frac{\left(r_{t}-\alpha-\beta x_{t-1}\right)\left(x_{t}-\gamma-\rho x_{t-1}\right)}{\sigma_{u} \sigma_{e}}\right.\\ &\left.+\frac{\left(x_{t}-\gamma-\rho x_{t-1}\right)^{2}}{\sigma_{e}^{2}}\right] \end{aligned}
\end{equation}
and observe that the t-test squared is equal to the likelihood ratio test statistic
\begin{equation}
\max _{\beta, \rho, \alpha, \gamma} L(\beta, \rho, \alpha, \gamma)-\max _{\rho, \alpha, \gamma} L\left(\beta_{0}, \rho, \alpha, \gamma\right)=t\left(\beta_{0}\right)^{2}
\end{equation}
But this test statistic turns out to be the same if only the marginal log likelihood 
\begin{equation}
L(\beta, \alpha)=-\sum_{t=1}^{T}\left(r_{t}-\alpha-\beta x_{t-1}\right)^{2}
\end{equation}
is used in its computation. Thus the t-test ignores information about $\rho$.

To reason about how to incorporate information about $\rho$ into the test Campbell and Yogo (2006) assume $\rho$ to be known at first. If, further, the assumption $\alpha=\gamma=0$ is made, the only unknown variable that remains is $\beta$. Now the likelihood function can be denoted as $L(\beta)$. If one restricts oneself to consider only the simple alternative of the form $\beta=\beta_{1}$ one can reason by the Neyman–Pearson Lemma that the most powerful test is of the form
\begin{equation}
\begin{aligned} \sigma_{u}^{2}\left(1-\delta^{2}\right)\left(L\left(\beta_{1}\right)-L\left(\beta_{0}\right)\right)=& 2\left(\beta_{1}-\beta_{0}\right) \sum_{t=1}^{T} x_{t-1}\left[r_{t}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)\right] \\ &-\left(\beta_{1}^{2}-\beta_{0}^{2}\right) \sum_{t=1}^{T} x_{t-1}^{2}>C \end{aligned}
\end{equation}
with $\beta_{u e}=\sigma_{u e} / \sigma_{e}^{2}$ and $C$ being some constant.
This optimal test is not uniformly most powerful (UMP), however, since it is a weighted sum of minimal sufficient statistics and the weights depend on $\beta_{1}$. For that reason, Campbell and Yogo (2006) propose to condition the test on the ancillary statistic  $\sum_{t=1}^{T} x_{t-1}^{2}$. As a result the test can be simplified to
\begin{equation}
 \sum_{t=1}^{T} x_{t-1}\left[r_{t}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)\right]>C
\end{equation}
which is UMP for alternatives of the form $\beta_{1}>\beta_{0}$ when $\rho$ is known.

For the test-statistic to have a standard normal distribution under the null Campbell and Yogo (2006) propose to recenter and rescale which results in 
\begin{equation}
\frac{\sum_{t=1}^{T} x_{t-1}\left[r_{t}-\beta_{0} x_{t-1}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)\right]}{\sigma_{u}\left(1-\delta^{2}\right)^{1 / 2}\left(\sum_{t=1}^{T} x_{t-1}^{2}\right)^{1 / 2}}>C
\end{equation}
Revoking the previous assumption of $\alpha=\gamma=0$ and generalizing instead to any unknown  $\alpha$ and $\gamma$ by replacing $x_{t-1}$ by its demeaned value $x_{t-1}^{\mu}$, the test
\begin{equation}
Q\left(\beta_{0}, \rho\right)=\frac{\sum_{t=1}^{T} x_{t-1}^{\mu}\left[r_{t}-\beta_{0} x_{t-1}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)\right]}{\sigma_{u}\left(1-\delta^{2}\right)^{1 / 2}\left(\sum_{t=1}^{T} x_{t-1}^{\mu 2}\right)^{1 / 2}}>C
\end{equation}
is UMP conditional on $\sum_{t=1}^{T} x_{t-1}^{\mu 2}$.
This Q-statistic, as Campbell and Yogo call it, has an intuitive interpretations when $\beta_{0} = 0$. It is the t-statistic of the $\beta^{\star}$ coefficient in the regression $r_{t}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)=\alpha^{\star}+\beta^{\star} x_{t-1}+v_{t}$. This regression can be interpreted as regressing the de-noised returns onto the regressor $x_{t-1}$ where we exploit the information contained in $\rho$ and the correlation of the shocks.
When $\beta_{u e}=0$, i.e., the correlation of the shocks is zero, the Q-statistic simplifies to the t-statistic which converges to a standard-normal distribution in this case. When $\beta_{u e}!=0$, the t-statistic does not converge to a standard-normal distribution. Instead, as shown by Elliott and Stock (1994), under local-to-unity asymptotic theory it has the null distribution 
\begin{equation}
t\left(\beta_{0}\right) \Rightarrow \delta \frac{\tau_{c}}{\kappa_{c}}+\left(1-\delta^{2}\right)^{1 / 2} Z
\end{equation}
where $\left(W_{u}(s), W_{e}(s)\right)^{\prime}$ is a two-dimensional Wiener process with correlation $\delta$, $J_{c}(s)$ is defined by $\mathrm{d} J_{c}(s)=c J_{c}(s) \mathrm{d} s+\mathrm{d} W_{e}(s)$ with $J_{c}(0)=0$, $Z$ is a standard normal random variable independent of $\left(W_{e}(s), J_{c}(s)\right)$, $\kappa_{c}=\left(\int J_{c}^{\mu}(s)^{2} \mathrm{d} s\right)^{1 / 2}$, $\tau_{c}=\int J_{c}^{\mu}(s) \mathrm{d} W_{e}(s)$.

Within the local-to-unity asymptotic theory the persistence of the process $x_{t}$ is modeled as $\rho=1+c / T$ where $c$ is a constant and $T$ is the sample size. Hence, when $c<0$, $x_{t}$ is $I(0)$ but highly persistent and its sample moments do not converge in probability to constants but to functionals of a diffusion process instead.  The t-test is not feasible since it depends on the unknown parameter $\rho$ through $\tau_{c} / \kappa_{c}$ which also makes it non-standard. 

\subsection{A pretest}
From equation ... it's easy to see that if $\delta = 0$ the nuisance term $\tau_{c} / \kappa_{c}$ vanishes and the t-statistic collapses to the standard normal  random variable $Z$. It therefore makes sense to test whether $\delta$ is different from zero. If it isn't, one can infer that the t-statistic is approximately standard normal and inference based on it is valid. This, however, is not the only condition in which there is no size distortion. Phillips (1987) shows that if $x_{t}$ is not persistent, $\tau_{c} / \kappa_{c}$ converges to a different standard normal random variable $Z^{\star}$. Hence the t-statistic, as a sum of two independent standard normal random variables, is likewise standard normal. 

Campbell and Yogo (2006) use these facts as bases for a pretest. This test supposes that an actual size $\alpha^{\star}  \geq \alpha$ may be acceptable if its not much greater than the nominal size $\alpha$. It then tests whether $\alpha^{\star}$ is greater than a prespecified acceptable size. It would be tempting to use a unit root test and infer no size distortion if its null of non-stationarity can be rejected. Elliott and Stock (1994) show, however, that this does not guarantee valid inference via the t-statistic. Instead, Campbell and Yogo (2006) use a unit root test statistic to construct a confidence interval for $c$ by inverting its alternative distribution in the spirit of Stock (1991). $\delta$, on the other hand, can be consistently estimated from the residuals of regression ($\ref{eqn:1}$, $\ref{eqn:2}$). The size of the t-statistic forms a two-dimensional surface in the $c$-$\delta$-parameter space. If the confidence interval for $c$ lies strictly outside the region where the size is above the acceptable threshold for $\hat{\delta}$, the null of unacceptable size distortion can be rejected. This test is implemented with tables provided by Campbell and Yogo (2006) which use the DF-GLS test statistic proposed by Elliott et al. (1996) to construct the confidence interval for $c$. My application uses the closest tabulated values to the estimated values without interpolating.

\subsection{Making the Q-test feasible}
When the pretest cannot reject the null of unacceptable size distortion we cannot use the unadjusted t-test to do inference. While the Q-test is UMP when $\rho$ and $\delta$ are known, in practice this is not the case. Furthermore, $\rho$ cannot be estimated consistently since $\operatorname{Var}\left(x_{t}\right)=\frac{\sigma_{e}^{2}}{1-\rho^{2}}$ gets large if $\rho$ is local to unity. Instead, as described in the previous section, Campbell and Yogo (2006) construct a $100\left(1-\alpha_{1}\right) \%$ confidence interval, $C_{\rho}\left(\alpha_{1}\right)$ for $\rho$ from a unit root test statistic. They then construct $C_{\beta | \rho}\left(\alpha_{2}\right)$, a $100\left(1-\alpha_{2}\right) \%$ conditional on $\rho$ confidence interval for $\beta$. To marginalize $\rho$ and thus getting an unconditional confidence interval for $\beta$ they take the union over $\rho \in C_{\rho}\left(\alpha_{1}\right)$
\begin{equation}
C_{\beta}(\alpha)=\bigcup_{\rho \in C_{\rho}\left(\alpha_{1}\right)} C_{\beta | \rho}\left(\alpha_{2}\right)
\end{equation}

$C_{\beta}(\alpha)$ has coverage of at least $100(1-\alpha) \%$ with $\alpha=\alpha_{1}+\alpha_{2}$. This follows from Bonferroni's inequality which gives this method its name.  
When the regressor is a valuation ratio, we should expect $\delta$ to be negative. Further, note that $\beta(\rho)$ is given by 
\begin{equation}
\beta(\rho)=\frac{\sum_{t=1}^{T} x_{t-1}^{\mu}\left[r_{t}-\beta_{u e}\left(x_{t}-\rho x_{t-1}\right)\right]}{\sum_{t=1}^{T} x_{t-1}^{\mu 2}}
\end{equation}
Then the estimate of $\beta$ declines linearly in $\rho$ and its confidence interval is 
\begin{equation}
C_{\beta}(\alpha)=\left[\underline{\beta}\left(\overline{\rho}\left(\overline{\alpha}_{1}\right), \alpha_{2}\right), \overline{\beta}\left(\underline{\rho}\left(\underline{\alpha}_{1}\right), \alpha_{2}\right)\right]
\end{equation}
with $C_{\rho}\left(\alpha_{1}\right)= \left[\underline{\rho}\left(\underline{\alpha}_{1}\right), \overline{\rho}\left(\overline{\alpha}_{1}\right)\right]$ being the confidence interval for $\rho$, $\underline{\alpha}_{1}=\operatorname{Pr}\left(\rho<\underline{\rho}\left(\underline{\alpha}_{1}\right)\right)$, $\overline{\alpha}_{1}=\operatorname{Pr}\left(\rho>\overline{\rho}\left(\overline{\alpha}_{1}\right)\right)$, and $\alpha_{1}=\underline{\alpha}_{1}+\overline{\alpha}_{1}$. We also know that the Q-statistic is normally distributed. Let $z_{\alpha_{2} / 2}$ denote the $1-\alpha_{2} / 2$ quantile of the standard normal distribution. Then the lower bound is given by
\begin{equation}
\underline{\beta}\left(\overline{\rho}\left(\overline{\alpha}_{1}\right), \alpha_{2}\right)=\beta \left(\overline{\rho}\left(\overline{\alpha}_{1}\right) \right)-z_{\alpha_{2} / 2} \sigma_{u}\left(\frac{1-\delta^{2}}{\sum_{t=1}^{T} x_{t-1}^{\mu}}\right)^{1 / 2}
\end{equation}
and the upper bound is given by
\begin{equation}
\overline{\beta}\left(\underline{\rho}\left(\underline{\alpha}_{1}\right), \alpha_{2}\right)=\beta \left(\underline{\rho}\left(\underline{\alpha}_{1}\right) \right)+z_{\alpha_{2} / 2} \sigma_{u}\left(\frac{1-\delta^{2}}{\sum_{t=1}^{T} x_{t-1}^{\mu 2}}\right)^{1 / 2}
\end{equation}

If this confidence interval does not contain zero, we infer that the regressor has predictive power. While this test is valid, it is conservative, however, since Bonferroni's inequality is likely to be strict i.e. the coverage of $C_{\beta}(\alpha)$ is likely greater than $100(1-\alpha) \%$. For that reason Campbell and Yogo (2006) refine the confidence interval based on a numerical method proposed by Cavanagh et al. (1995). To obtain a tighter confidence interval with significance level $\widetilde{\alpha}$ according to this method $\alpha_{2}$ is fixed first. $\operatorname{Pr}\left(\underline{\beta}\left(\overline{\rho}\left(\overline{\alpha}_{1}\right), \alpha_{2}\right)>\beta\right)$ is then evaluated for each $\delta$ and $\overline{\alpha}_{1}$ is selected such that 
\begin{equation}
\operatorname{Pr}\left(\underline{\beta}\left(\overline{\rho}\left(\overline{\alpha}_{1}\right),\alpha_{2}\right)>\beta\right) \leq \widetilde{\alpha} / 2\end{equation}
holds for all $c$ and, importantly,
\begin{equation}
\operatorname{Pr}\left(\underline{\beta}\left(\overline{\rho}\left(\overline{\alpha}_{1}\right), \alpha_{2}\right)>\beta\right) = \widetilde{\alpha} / 2
\end{equation}
holds for some $c$. 
Similarly, $\underline{\alpha}_{1}$ is selected such that 
\begin{equation}
\operatorname{Pr}\left(\overline{\beta}\left(\underline{\rho}\left(\underline{\alpha}_{1}\right), \alpha_{2}\right)<\beta\right) \leq \widetilde{\alpha} / 2
\end{equation}
holds again for all $c$ with equality at some $c$. A one-sided Bonferroni test based a confidence interval thus obtained has size $\widetilde{\alpha} / 2$ for some permissible $c$ while a two-sided test has at most size  $\widetilde{\alpha}$ .  

\subsection{Implementation of the feasible Q-test}
\label{implementation}

The feasible Q-test, generalized to less restrictive assumptions, is implemented in the R programming language following the steps proposed by Campbell and Yogo (2005). Suppose the regressor follows an $AR(p)$ process. Further, the innovations are not necessarily normally distributed but are a martingale difference sequence, i.e., 
\begin{equation}
\mathrm{E}\left[w_{t} |\mathscr{F}_{t-1}\right]=0
\end{equation}
where $\mathscr{F}_{t}=\left\{w_{s} | s \leq t\right\}$ denotes the filtration generated by $w_{t}=\left(u_{t}, e_{t}\right)^{\prime}$.
Its fourth moments are assumed to be finite and the unconditional variance is fixed, i.e.,
\begin{equation}
\begin{array}{l}{\mathrm{E}\left[w_{t} w_{t}^{\prime}\right]=\Sigma} \\ {\sup _{t} \mathrm{E}\left[u_{t}^{4}\right]<\infty, \sup _{t} \mathrm{E}\left[t_{t}^{4}\right]<\infty, \text { and } \mathrm{E}\left[x_{0}^{2}\right]<\infty}\end{array}
\end{equation}
The dynamics of $x_{t}$ are written as
\begin{equation}
\label{eqn:22}
x_{t}=\gamma+\rho x_{t-1}+v_{t}
\end{equation}
where $\rho$ is the largest autoregressive root and $b(L) v_{t}=e_{t}$ with $b(L)=\sum_{i=0}^{p-1} b_{i} L^{i}$, $b_{0}=1$, and $b(1) \neq 0$. In this generalized case, the Q-statistic is not asymptotically standard normal distributed under the null. This is corrected by modifying it to
\begin{equation}
Q\left(\beta_{0}, \rho\right)=\frac{\sum_{t=1}^{T} x_{t-1}^{\mu}\left[r_{t}-\beta_{0} x_{t-1}-\sigma_{u e} /\left(\sigma_{e} \omega\right)\left(x_{t}-\rho x_{t-1}\right)\right]+\frac{T}{2} \sigma_{u e} /\left(\sigma_{e} \omega\right)\left(\omega^{2}-\sigma_{v}^{2}\right)}{\sigma_{u}\left(1-\delta^{2}\right)^{1 / 2}\left(\sum_{t=1}^{T} x_{t-1}^{\mu 2}\right)^{1 / 2}}
\end{equation}
where $\omega=\sigma_{e} / b(1)$.

First $\hat{\beta}$ and its standard error $\operatorname{SE}(\widehat{\beta})$ are obtained by OLS estimation of equation (1). $x_{t}$ is written in its augmented Dickey-Fuller form
\begin{equation}
\Delta x_{t}=\tau+\theta x_{t-1}+\sum_{i=1}^{p-1} \psi_{i} \Delta x_{t-i}+e_{t}
\end{equation}
where $\psi_{i}=-\sum_{j=i}^{p-1} a_{j}$, $a(L)=L^{-1}[1-(1-\rho L) b(L)]$, and $\theta=(\rho-1) b(1)$. Estimation of $\widehat{\psi}_{i}(i=1, \ldots, p-1)$ is done via OLS where $p$ is either explicitly specified or estimated via the Bayesian information criterion [BIC].
With $\widehat{u}_{t}$ and $\widehat{e}_{t}$ being the residuals of regression (1) and (2), respectively,
\begin{align} \widehat{\sigma}_{u}^{2} &=\frac{1}{T-2} \sum_{t=1}^{T} \widehat{u}_{t}^{2} \\ \widehat{\sigma}_{e}^{2} &=\frac{1}{T-2} \sum_{t=1}^{T} \widehat{e}_{t}^{2} \\ \widehat{\sigma}_{u e} &=\frac{1}{T-2} \sum_{t=1}^{T} \widehat{u}_{t} \widehat{e}_{t} \\ \widehat{\delta} &=\frac{\widehat{\sigma}_{u e}}{\widehat{\sigma}_{u} \widehat{\sigma}_{e}} \\ \widehat{\omega}^{2}&=\widehat{\sigma}_{e}^{2} /\left(1-\sum_{i=1}^{p-1} \widehat{\psi}_{i}\right)^{2}
\end{align}
are then computed. OLS estimation of equation ($\ref{eqn:22}$) yields $\widehat{\rho}$ and its standard error $\mathrm{SE}(\widehat{\rho})$. The residual $\widehat{v}_{t}$ is used to compute 
\begin{equation}
\widehat{\sigma}_{v}^{2}=(T-2)^{-1} \sum_{t=1}^{T} \widehat{v}_{t}^{2}
\end{equation} 
Now, to compute the DF-GLS statistic, $\left(x_{0}, x_{1}-\rho_{G L S} x_{0}, \ldots, x_{T}-\rho_{G L S} x_{T-1}\right)^{\prime}$ is regressed onto 
$\left(1,1-\rho_{G L S}, \ldots, 1-\rho_{G L S}\right)^{\prime}$ with $\rho_{G L S}=1-7 / T$. The coefficient $\mu_{G L S}$ is used to compute $\overline{x}_{t}=x_{t}-\mu_{G L S}$ which, in turn, is needed for the regression 
\begin{equation}
\Delta \overline{x}_{t}=\theta \overline{x}_{t-1}+\sum_{i=1}^{p-1} \psi_{i} \Delta \overline{x}_{t-i}+e_{t}
\end{equation} which is estimated with no intercept. Finally, the t-statistic for $\theta$ is computed which is the DF-GLS statistic. To confirm that no error is conducted in this estimation procedure, the DF-GLS statistic thus obtained is compared to the implementation in the urca package. 

$[\underline{\rho}, \overline{\rho}]=[1+\underline{c} / T, 1+\overline{c} / T]$ is obtained by first finding $[\underline{c}, \overline{c}]$ via tables 2–11 of Campbell and Yogo (2005) where the closest values to the estimated DF-GLS statistic and $\hat{\delta}$ are used. Let a temporary variable $r^{\star}$ be defined by
\begin{equation}
r_{t}^{*}=r_{t}-\widehat{\sigma}_{u e} \widehat{\sigma}_{e}^{-2}\left(x_{t}-\rho x_{t-1}\right).
\end{equation}
Now the regression
\begin{equation}
r_{t}^{*}=\alpha+\beta x_{t-1}+u_{t}
\end{equation} 
is run for each $\rho=\{\underline{\rho}, \overline{\rho}\}$ to get $\widehat{\beta}(\rho)$. $[\underline{\beta}(\rho), \overline{\beta}(\rho)]$ is then obtained by computing

\begin{equation}
\underline{\beta}(\rho)=\widehat{\beta}(\rho)+\frac{T-2}{2} \frac{\widehat{\sigma_{ue}}}{\widehat{\sigma}_{e} \widehat{\omega}}\left(\frac{\widehat{\omega}^{2}}{\widehat{\sigma}_{v}^{2}}-1\right) \operatorname{SE}(\widehat{\rho})^{2}-1.645\left(1-\widehat{\delta}^{2}\right)^{1 / 2} \operatorname{SE}(\widehat{\beta})
\end{equation}
and 
\begin{equation}
\overline{\beta}(\rho)=\widehat{\beta}(\rho)+\frac{T-2}{2} \frac{\widehat{\sigma}_{u e}}{\widehat{\sigma}_{e} \widehat{\omega}}\left(\frac{\widehat{\omega}^{2}}{\widehat{\sigma}_{v}^{2}}-1\right) \operatorname{SE}(\widehat{\rho})^{2}+1.645\left(1-\widehat{\delta}^{2}\right)^{1 / 2} \operatorname{SE}(\widehat{\beta})
\end{equation}
A 5\% one-sided or 10\% two-sided test of $H_0: \beta = 0$ is based on the 90\% Bonferroni confidence interval
$$[\underline{\beta}(\overline{\rho}), \overline{\beta}(\underline{\rho})]$$

\section{Monte Carlo simulation}

To confirm the correct implementation of the procedure, Table~\vref{tab:cy} replicates Table 3 of Campbell and Yogo (2006) with the same input parameters. For each parameter combination 10,000 Monte Carlo samples are drawn from a multivariate standard normal distribution with correlation $\delta$. These variates represent the innovations, which are used to simulate sample paths according to system ($\ref{eqn:1}$, $\ref{eqn:2}$). The null hypothesis $H_0: \beta = 0$ is tested against the alternative $H_1: \beta > 0$ at the 5\% significance level. If the Bonferroni confidence interval lies strictly above zero, the one-sided test is rejected. 
\begin{table}[hb!]
\caption{Finite-sample rejection rates of the true null for right-tailed tests of predictability at $\alpha$=0.05 (10,000 Monte Carlo samples).}
\label{tab:cy}
\centering
\begin{tabular}{rrrrrrrr}
 \hline
 & Obs & $c$ & $\rho$ & $\delta$ & t-test & Bonf. Q-test & Q-test \\ 
  \hline
1 & 50 & 0 & 1.000 & -0.95 & 0.4160 & 0.0826 & 0.0483 \\ 
  2 & 50 & 0 & 1.000 & -0.75 & 0.2916 & 0.0837 & 0.0515 \\ 
  3 & 50 & -2 & 0.961 & -0.95 & 0.2714 & 0.0868 & 0.0482 \\ 
  4 & 50 & -2 & 0.961 & -0.75 & 0.2079 & 0.0881 & 0.0532 \\ 
  5 & 50 & -20 & 0.608 & -0.95 & 0.0977 & 0.1206 & 0.0515 \\ 
  6 & 50 & -20 & 0.608 & -0.75 & 0.0840 & 0.1078 & 0.0484 \\ 
  7 & 100 & 0 & 1.000 & -0.95 & 0.4217 & 0.0616 & 0.0480 \\ 
  8 & 100 & 0 & 1.000 & -0.75 & 0.2930 & 0.0616 & 0.0497 \\ 
  9 & 100 & -2 & 0.980 & -0.95 & 0.2698 & 0.0587 & 0.0505 \\ 
  10 & 100 & -2 & 0.980 & -0.75 & 0.2104 & 0.0588 & 0.0489 \\ 
  11 & 100 & -20 & 0.802 & -0.95 & 0.1063 & 0.0622 & 0.0471 \\ 
  12 & 100 & -20 & 0.802 & -0.75 & 0.0874 & 0.0514 & 0.0500 \\ 
  13 & 250 & 0 & 1.000 & -0.95 & 0.4259 & 0.0476 & 0.0483 \\ 
  14 & 250 & 0 & 1.000 & -0.75 & 0.2970 & 0.0506 & 0.0536 \\ 
  15 & 250 & -2 & 0.992 & -0.95 & 0.2866 & 0.0507 & 0.0481 \\ 
  16 & 250 & -2 & 0.992 & -0.75 & 0.2092 & 0.0466 & 0.0492 \\ 
  17 & 250 & -20 & 0.920 & -0.95 & 0.1080 & 0.0406 & 0.0517 \\ 
  18 & 250 & -20 & 0.920 & -0.75 & 0.0944 & 0.0369 & 0.0501 \\ 
   \hline
\end{tabular}
\end{table}

Next, slightly different input parameters are chosen. If the results differ qualitatively, the previous findings might be the result of selection bias. As demonstrated by Table~\vref{tab:cy_changed}, no such suspicion is warranted. As long as the sample size is at least 100 and $\rho$ is local to unity, the Bonferroni Q-test has acceptable finite sample rejection rates. The infeasible Q-test, unsurprisingly, always has rejection rate close to $\alpha$. The t-test, on the other hand, significantly over-rejects when the innovations are highly correlated and $\rho$ is local to unity. It should be noted, however, that the two-sided Bonferroni Q-test tends to under-reject for the chosen parameters. 
\begin{table}[h!]
\caption{Finite-sample rejection rates  of the true null for right-tailed and two-tailed tests of predictability at $\alpha$=0.05 and $\alpha$=0.1, respectively (10,000 Monte Carlo samples).}
\centering
\label{tab:cy_changed}
\begin{tabular}{rrrrrrrrr}
  \hline
 & Obs & $c$ & $\rho$ & $\delta$ & t-test & Bonf. Q-test & Q-test & Bonf. Q-test (two-sided) \\ 
  \hline
1 & 50 & 0 & 1.000 & -0.99 & 0.4442 & 0.0863 & 0.0544 & 0.0864 \\ 
  2 & 50 & 0 & 1.000 & -0.90 & 0.3786 & 0.0943 & 0.0506 & 0.0953 \\ 
  3 & 50 & -1 & 0.980 & -0.99 & 0.3502 & 0.0857 & 0.0486 & 0.0898 \\ 
  4 & 50 & -1 & 0.980 & -0.90 & 0.3107 & 0.0949 & 0.0489 & 0.0964 \\ 
  5 & 50 & -10 & 0.804 & -0.99 & 0.1440 & 0.0821 & 0.0512 & 0.1004 \\ 
  6 & 50 & -10 & 0.804 & -0.90 & 0.1226 & 0.0840 & 0.0472 & 0.0929 \\ 
  7 & 50 & -15 & 0.706 & -0.99 & 0.1114 & 0.0895 & 0.0485 & 0.1112 \\ 
  8 & 50 & -15 & 0.706 & -0.90 & 0.1035 & 0.0981 & 0.0468 & 0.1112 \\ 
  9 & 150 & 0 & 1.000 & -0.99 & 0.4467 & 0.0508 & 0.0508 & 0.0509 \\ 
  10 & 150 & 0 & 1.000 & -0.90 & 0.3804 & 0.0524 & 0.0494 & 0.0554 \\ 
  11 & 150 & -1 & 0.993 & -0.99 & 0.3584 & 0.0475 & 0.0493 & 0.0611 \\ 
  12 & 150 & -1 & 0.993 & -0.90 & 0.3137 & 0.0572 & 0.0532 & 0.0625 \\ 
  13 & 150 & -10 & 0.934 & -0.99 & 0.1510 & 0.0468 & 0.0480 & 0.0720 \\ 
  14 & 150 & -10 & 0.934 & -0.90 & 0.1309 & 0.0534 & 0.0484 & 0.0672 \\ 
  15 & 150 & -15 & 0.901 & -0.99 & 0.1273 & 0.0446 & 0.0520 & 0.0819 \\ 
  16 & 150 & -15 & 0.901 & -0.90 & 0.1133 & 0.0484 & 0.0489 & 0.0695 \\ 
  17 & 200 & 0 & 1.000 & -0.99 & 0.4477 & 0.0435 & 0.0478 & 0.0438 \\ 
  18 & 200 & 0 & 1.000 & -0.90 & 0.3786 & 0.0501 & 0.0520 & 0.0546 \\ 
  19 & 200 & -1 & 0.995 & -0.99 & 0.3582 & 0.0449 & 0.0483 & 0.0607 \\ 
  20 & 200 & -1 & 0.995 & -0.90 & 0.3127 & 0.0542 & 0.0468 & 0.0615 \\ 
  21 & 200 & -10 & 0.950 & -0.99 & 0.1490 & 0.0401 & 0.0497 & 0.0666 \\ 
  22 & 200 & -10 & 0.950 & -0.90 & 0.1352 & 0.0486 & 0.0502 & 0.0634 \\ 
  23 & 200 & -15 & 0.925 & -0.99 & 0.1276 & 0.0369 & 0.0487 & 0.0786 \\ 
  24 & 200 & -15 & 0.925 & -0.90 & 0.1234 & 0.0482 & 0.0491 & 0.0670 \\ 
   \hline
\end{tabular}
\end{table}

As argued by Phillips (2014), the Q-test as derived in section (3) is asymptotically invalid when the largest autoregressive root of the regressor is not local to unity. In the following I will demonstrate the finite sample behavior as $\rho$ diverges from unity. Again 10,000 Monte Carlo simulations are run with Gaussian innovations. A relatively small sample size is chosen to keep the DF-GLS statistic small in absolute value and thus stay in the vicinity of the implementation of Campbell and Yogo (2005). Table~\vref{tab:non-local} shows that as $\rho$ distances itself from the unity neighborhood the Bonferroni Q-test significantly over-rejects the true null of no predictability. 
\begin{table}[h!]
\caption{Finite-sample rejection rates of the true null for right-tailed and two-tailed tests of predictability at $\alpha$=0.05 and $\alpha$=0.1, respectively, with non local to unity autoregressive root (10,000 Monte Carlo samples). }
\label{tab:non-local}
\centering
\begin{tabular}{rrrrrrrrr}
  \hline
 & Obs & $c$ & $\rho$ & $\delta$ & t-test & Bonf. Q-test & Q-test & Bonf. Q-test (two-sided) \\ 
  \hline
1 & 50 & 0 & 1.000 & -0.95 & 0.4147 & 0.0946 & 0.0480 & 0.0957 \\ 
  2 & 50 & 0 & 1.000 & -0.75 & 0.2810 & 0.0838 & 0.0475 & 0.0870 \\ 
  3 & 50 & 0 & 1.000 & -0.50 & 0.1758 & 0.0846 & 0.0536 & 0.0940 \\ 
  4 & 50 & -5 & 0.902 & -0.95 & 0.1887 & 0.0896 & 0.0499 & 0.0980 \\ 
  5 & 50 & -5 & 0.902 & -0.75 & 0.1487 & 0.0830 & 0.0522 & 0.0891 \\ 
  6 & 50 & -5 & 0.902 & -0.50 & 0.1081 & 0.0701 & 0.0512 & 0.0811 \\ 
  7 & 50 & -10 & 0.804 & -0.95 & 0.1325 & 0.0894 & 0.0543 & 0.1018 \\ 
  8 & 50 & -10 & 0.804 & -0.75 & 0.1126 & 0.0789 & 0.0462 & 0.0867 \\ 
  9 & 50 & -10 & 0.804 & -0.50 & 0.0887 & 0.0689 & 0.0510 & 0.0820 \\ 
  10 & 50 & -20 & 0.608 & -0.95 & 0.0987 & 0.1266 & 0.0506 & 0.1469 \\ 
  11 & 50 & -20 & 0.608 & -0.75 & 0.0906 & 0.1087 & 0.0506 & 0.1216 \\ 
  12 & 50 & -20 & 0.608 & -0.50 & 0.0682 & 0.0800 & 0.0443 & 0.0948 \\ 
  13 & 50 & -30 & 0.412 & -0.95 & 0.0811 & 0.1666 & 0.0455 & 0.1961 \\ 
  14 & 50 & -30 & 0.412 & -0.75 & 0.0787 & 0.1272 & 0.0500 & 0.1460 \\ 
  15 & 50 & -30 & 0.412 & -0.50 & 0.0719 & 0.1003 & 0.0523 & 0.1195 \\ 
   \hline
\end{tabular}
\end{table}

Campbell and Yogo (2006) evaluate the robustness of the Q-test under a fat-tailed distribution. In particular, they find finite sample rejection probabilities to be unchanged from the Gaussian case when the innovations follow a Student t-distribution with five degrees of freedom. As shown by Cavaliere (2004) unit root tests may be invalid under conditional heteroskedasticity. Assumptions ... imply validity of the Q-test under conditional heteroskedasticity only if the variance is covariance stationary. 
%Jacquier et al. (2004), however, show that this assumption might be too restrictive to adequately model the dynamics in reality. In the following I will show finite sample rejection rates when the ... modeled as a BEKK GARCH(1,1) process approaches nonstationarity.
It is therefore interesting to observe the behavior of the Bonferroni Q-test if innovations are modeled by a generalized autoregressive conditional heteroskedasticity [GARCH] process that approaches non-stationarity. If the regressor is a valuation ratio, it seems sensible that there is a more or less stable correlation between the regressor and stock returns. A multivariate GARCH model that allows such a restriction is the Constant Conditional Correlations GARCH  [CCC-GARCH] model of Bollerslev (1990). Let $ \boldsymbol{z_{t}}$ be a bivariate standard normal random variable, i.e., $ \boldsymbol{z}_{t} \stackrel{i i d}{\sim} \mathrm{N}\left(\mathbf{0}, \boldsymbol{I}_{2}\right)$. Then the innovations are modeled as 
\begin{equation}
 \boldsymbol{w}_{t}=\boldsymbol{H}_{t}^{1 / 2} \boldsymbol{z}_{t}
\end{equation}
where $ \boldsymbol{H_t}$ denotes the covariance matrix at time $t$. $\boldsymbol{H}_{t}$ is a positive definite matrix whose Cholesky decomposition factors into 
\begin{equation}
\boldsymbol{H}_{t}^{1/2} = \boldsymbol{D}_{t} \boldsymbol{R}_{t}^{1 / 2}
\end{equation}
with $\boldsymbol{D}_{t}$ being a diagonal matrix with the conditional standard deviations 
\begin{equation}
\label{eqn:40}
\boldsymbol{D}_{t}=\left[\begin{array}{cc}{\sqrt{h_{1 t}^{2}}} & {0} \\ {0} & {\sqrt{h_{2 t}^{2}}}\end{array}\right].
\end{equation}
and $\boldsymbol{R}_{t}$ being the positive definite conditional correlation matrix 
\begin{equation}
\boldsymbol{R}_{t}=\left[\begin{array}{cc}{1} & {\delta_{ t}} \\ {\delta_{ t}} & {1}\end{array}\right]
\end{equation}
Since correlations are assumed to be constant through time, $\boldsymbol{R}_{t}=\boldsymbol{R}$. The conditional variances 
$${h}_{i t}^{2}, \quad t=1, \ldots, T, \quad i=1, 2$$
in $\boldsymbol{D}_{t}$ are modeled by independent univariate GARCH processes, i.e.,
\begin{equation}
\begin{aligned} \epsilon_{t} &=h_{t} \eta_{t}, \quad \eta_{t} \stackrel{i i d}{\sim}(0,1) \\ h_{t}^{2} &=\omega+\sum_{i=1}^{q} \alpha_{i} \epsilon_{t-i}^{2}+\sum_{i=1}^{p} \beta_{i} h_{t-i}^{2} \\ \omega &>0, \quad \alpha_{i} \geq 0, \quad i=1, \ldots, q, \quad \beta_{i} \geq 0, \quad i=1, \ldots, p \end{aligned}
\end{equation}
Simulation results are shown in Table~\vref{tab:ccc-garch}.
\begin{table}[ht]
\centering
\caption{Finite-sample rejection rates of the true null for right-tailed  [BQ] and two-tailed  [BQ$_{{2sided}}$] Bonferroni Q-tests of predictability at $\alpha$=0.05 and $\alpha$=0.1, respectively, with innovations following a CCC-GARCH(1, 1) process (10,000 Monte Carlo samples). }
\label{tab:ccc-garch}
\begin{tabular}{rrrrrrrrrrr}
  \hline
 & Obs & $c$ & $\rho$ & $\delta$ &$\hat{\delta}$ & $\alpha_{_{GARCH}}$ & $\beta_{_{GARCH}}$ & t-test &BQ& BQ$_{{2sided}}$ \\ 
  \hline
1 & 100 & -2 & 0.98 & -0.95 & -0.6002 & 0.0999 & 0.90 & 0.1819 & 0.0635 & 0.0697 \\ 
  2 & 100 & -2 & 0.98 & -0.95 & -0.6053 & 0.0999 & 0.80 & 0.1685 & 0.0633 & 0.0723 \\ 
  3 & 100 & -2 & 0.98 & -0.95 & -0.6080 & 0.0999 & 0.50 & 0.1677 & 0.0645 & 0.0744 \\ 
  4 & 100 & -2 & 0.98 & -0.95 & -0.6081 & 0.0999 & 0.40 & 0.1663 & 0.0596 & 0.0678 \\ 
  5 & 100 & -2 & 0.98 & -0.95 & -0.6077 & 0.0999 & 0.30 & 0.1616 & 0.0600 & 0.0696 \\ 
  6 & 100 & -2 & 0.98 & -0.95 & -0.6077 & 0.0999 & 0.20 & 0.1675 & 0.0610 & 0.0680 \\ 
  7 & 100 & -2 & 0.98 & -0.95 & -0.6086 & 0.0999 & 0.10 & 0.1741 & 0.0667 & 0.0742 \\ 
   \hline
\end{tabular}
\end{table}

In the CCC-GARCH model it has been assumed that there are no volatility transmissions between the returns and the regressor. This assumption seems unrealistic and it reduces the correlation between the residuals as demonstrated by $\hat{\delta}$. There are various multivariate GARCH models suited for incorporating spill-over effects such as the BEKK model of Engle and Kroner (1995). It seems plausible, though, that the variances of both the regressor and the returns share a common stochastic trend which may be imagined as the overall economic uncertainty. This seems reasonable since it is hard to imagine having high variance in returns without having high variance in valuation ratios and vice versa. Simulation is performed similar as in the CCC-GARCH model but now $h_{1, t} =h_{2, t}$ in equation ($\ref{eqn:40}$). Results in Table~\vref{tab:cst-garch} compared with Table~\vref{tab:cy}  indicate that the one-sided Bonferroni Q-test tends to over-reject the true null if innovations are modeled as described above.
\begin{table}[hbt]
\centering
\caption{Finite-sample rejection rates of the true null for right-tailed [BQ] and two-tailed  [BQ$_{{2sided}}$] Bonferroni Q-tests of predictability at $\alpha$=0.05 and $\alpha$=0.1, respectively, with the variances of innovations sharing a common stochastic trend which is modeled as a univariate GARCH(1, 1) process (10,000 Monte Carlo samples). }
\label{tab:cst-garch}
\begin{tabular}{rrrrrrrrrr}
  \hline
  & Obs & $c$ & $\rho$ & $\delta$ & $\alpha_{_{GARCH}}$ & $\beta_{_{GARCH}}$ & t-test &BQ& BQ$_{{2sided}}$ \\ 
  \hline
1 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.90 & 0.2815 & 0.0952 & 0.0990 \\ 
  2 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.80 & 0.2762 & 0.1009 & 0.1048 \\ 
  3 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.50 & 0.2706 & 0.0942 & 0.0977 \\ 
  4 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.40 & 0.2676 & 0.0961 & 0.1004 \\ 
  5 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.30 & 0.2628 & 0.0932 & 0.0972 \\ 
  6 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.20 & 0.2704 & 0.0908 & 0.0943 \\ 
  7 & 50 & -2 & 0.961 & -0.9500 & 0.0999 & 0.10 & 0.2677 & 0.0935 & 0.0984 \\ 
  8 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.90 & 0.3135 & 0.0733 & 0.0771 \\ 
  9 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.80 & 0.2812 & 0.0703 & 0.0771 \\ 
  10 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.50 & 0.2780 & 0.0683 & 0.0747 \\ 
  11 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.40 & 0.2797 & 0.0676 & 0.0742 \\ 
  12 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.30 & 0.2758 & 0.0664 & 0.0734 \\ 
  13 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.20 & 0.2746 & 0.0629 & 0.0703 \\ 
  14 & 100 & -2 & 0.980 & -0.9500 & 0.0999 & 0.10 & 0.2749 & 0.0656 & 0.0715 \\ 
   \hline
\end{tabular}
\end{table}

\section{Empirical Results}

Table~\vref{tab:cy_sp} replicates the key results of Tables 4 and 5 of Campbell and Yogo (2005). The Bonferroni Q-test is implemented as described in section $\ref{implementation}$. Apart from rounding errors, the results are nearly the same except for the following discrepancies. First, the DF-GLS statistics for $p>1$ are higher than reported by Campbell and Yogo (2005).  As previously mentioned, the DF-GLS implementation of section $\ref{implementation}$ was verified with a different open-source implementation. Second, based on the BIC, a lower $p$ of 1 was selected for monthly ldp. Investigations showed that the difference in BIC when $p=1$ compared with $p=2$ is very small. 
%Obtained from https://sites.google.com/site/motohiroyogo/home/publications/Predict_ Data.xls.
\begin{table}[ht]
\centering

\caption{Replicated model parameters from Campbell and Yogo (2005)}
\label{tab:cy_sp}
\begin{threeparttable}
\begin{tabular}{llrrlrrrrrl}
  \hline
dataset & $x$ & obs & $\hat{\delta}$ & $CI_{\rho}$ & DF-GLS & $p$ &t-stat & pt & $\hat{\beta}$ & $CI_{\beta}$ \\ 
  \hline
  SP\_A & lep & 123 & -0.96 & [0.786,0.931] & -2.888 & 1 & 2.76 & 0 & 0.127 & [0.043,0.225] \\ 
  SP\_A & ldp & 123 & -0.85 & [0.94,1.006] & -1.247 & 3 & 1.95 & 0 & 0.083 & [-0.024,0.136] \\ 
  CRSP\_A & lep & 77 & -0.96 & [0.778,0.96] & -2.229 & 1 & 2.77 & 0 & 0.162 & [0.04,0.273] \\ 
  CRSP\_A & ldp & 77 & -0.72 & [0.926,1.01] & -1.033 & 1 & 2.53 & 0 & 0.158 & [0.013,0.186] \\ 
  CRSP\_Q & lep & 305 & -0.99 & [0.944,0.992] & -2.191 & 1 & 2.91 & 0 & 0.047 & [0.011,0.066] \\ 
  CRSP\_Q & ldp & 305 & -0.94 & [0.962,0.999] & -1.696 & 1 & 2.06 & 0 & 0.034 & [-0.009,0.044] \\ 
  CRSP\_M & lep & 913 & -0.99 & [0.985,1] & -1.859 & 1 & 2.66 & 0 & 0.013 & [0.001,0.018] \\ 
  CRSP\_M & ldp & 913 & -0.95 & [0.99,1.001] & -1.433 & 1 & 1.70 & 0 & 0.008 & [-0.005,0.01] \\ 
\hline
\end{tabular}
 \begin{tablenotes}
\item Data is taken from Motohiro Yogo's website at https://drive.google.com/file/d/0BzR-ojpYuaFMcnZteHFyWUVIUFU/view. The regressor is the 10-year moving average earnings to current price ratio [lep] and the log dividend to price ratio [ldp] in logs. 
\end{tablenotes}
\end{threeparttable}
\end{table}


 \pagebreak
\begin{thebibliography}{}

\bibitem{} 
{Campbell, J. Y., Yogo, M. }
\textit{Efficient tests of stock return predictability.} Journal of Financial Economics 81, 27-60, 2006.
\bibitem{} 
{Elliott, G., Stock, J.H.} \textit{Inference in time series regression when the order of integration of a regressor is
unknown.} Econometric Theory 10, 672–700,  1994.
\bibitem{} 
Elliott, G., Rothenberg, T. J., \& Stock, J. H. (1996). Efficient Tests for an Autoregressive Unit Root. Econometrica, 64(4), 813–836.
\bibitem{} 
Jacquier, E., Polson, N.G., Rossi, P.E., 2004. Bayesian analysis of stochastic volatility
models with fat-tails and correlated errors. J. Econometrics 122, 185–212.
\bibitem{} 
Cavaliere, G., 2004. Unit root tests under time-varying variances. Econometric Rev.
23, 259–292.
\bibitem{} 
Phillips, P. C. B., 2014, On confdence Intervals for autoregressive roots and predictive regression. Econometrica, 82(3), 1177-1195.
\bibitem{} 
Bollerslev (1990): Modelling the coherence in short–run nominal exchange rates: a multivariate generalized ARCH model, Review of Economics and Statistics, 73, 498–505.
\bibitem{} 
Engle and Kroner (1995). Multivariate Simultaneous Generalized ARCH, Econometric Theory, 11, 122–150.
\end{thebibliography}


\end{document}
